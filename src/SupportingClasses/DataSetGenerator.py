"""
Created on May 23, 2019

@author: Daniel Konecki
"""
import os
import argparse
from re import compile
import numpy as np
from numpy import floor, mean, triu, nonzero
from multiprocessing import cpu_count
from Bio.Seq import Seq
from Bio import AlignIO
from Bio.Blast import NCBIXML
from Bio.SeqIO import write, parse
from Bio.SeqRecord import SeqRecord
from Bio.PDB.PDBList import PDBList
from Bio.PDB.PDBParser import PDBParser
from Bio.Alphabet.IUPAC import ExtendedIUPACProtein
from Bio.PDB.Polypeptide import three_to_one, is_aa
from Bio.Align.Applications import MuscleCommandline, ClustalwCommandline
from Bio.Blast.Applications import NcbiblastpCommandline
from dotenv import find_dotenv, load_dotenv
from SeqAlignment import SeqAlignment
from AlignmentDistanceCalculator import AlignmentDistanceCalculator
try:
    dotenv_path = find_dotenv(raise_error_if_not_found=True)
except IOError:
    dotenv_path = find_dotenv(raise_error_if_not_found=True, usecwd=True)
load_dotenv(dotenv_path)


class DataSetGenerator(object):
    """
    This class is meant to automate data set generation and improve reproducibility of papers from our lab which rely on
    PDB structures and alignments generated by BLASTING the proteins represented in those structures.

    Attributes:
        input_path (str/path): The path to the directory where the data for this data set should be stored. It is
        expected to contain at least one directory name ProteinLists which should contain files where each line denotes
        a separate PDB id (four letter code only).
        file_name (str): The file name of the list in the ProteinLists folder (described above) which will be used to
        generate the data set for analysis. The file is expected to have a single (four letter) PDB coe on each line.
        protein_data (dict): A dictionary to hold the protein data generated over the course of data set construction.
        Initially it only contains the PDB ids parsed in from the provided file_name as keys, referencing empty
        dictionaries as values.
    """

    def __init__(self, input_path):
        """
        Init

        This function overwrites the default init function for the DataSetGenerator class.

        Args:
            input_path (path): The path to the directory where the data for this data set should be stored. It is
            expected to contain at least one directory name ProteinLists which should contain files where each line
            denotes a separate PDB id (four letter code only).
        """
        if input_path is None:
            self.input_path = os.path.join(os.environ.get('PROJECT_PATH'), 'Input')
        else:
            self.input_path = input_path
        self.protein_list_path = os.path.join(self.input_path, 'ProteinLists')
        self.pdb_path = os.path.join(self.input_path, 'PDB')
        self.sequence_path = os.path.join(self.input_path, 'Sequences')
        self.blast_path = os.path.join(self.input_path, 'BLAST')
        self.filtered_blast_path = os.path.join(self.input_path, 'Filtered_BLAST')
        self.protein_data = None

    def build_pdb_alignment_dataset(self, protein_list_fn, num_threads=1, max_target_seqs=20000, e_value_threshold=0.05,
                                    database='nr', remote=False, min_fraction=0.7, min_identity=40, max_identity=98,
                                    msf=True, fasta=True):
        """
        Build Dataset

        This method builds a complete data set based on the protein id list specified in the constructor.

        Args:
            protein_list_fn (str): The name of the file where the list of PDB ids (five letter codes, first four
            characters are the PDB ID and the last character is the chain id) of which the data set consists can be
            found. Each id is expected to be on its own line.
            num_threads (int): The number of threads to use when performing the BLAST search.
            max_target_seqs (int): The maximum number of hits to look for in the BLAST database.
            e_value_threshold (float): The maximum e-value for a passing hit.
            database (str): The name of the database used to search for paralogs, homologs, and orthologs.
            remote (bool): Whether to perform the call to blastp remotely or not (in this case num_threads is ignored).
            min_fraction (float): The minimum fraction of the query sequence length for a passing hit.
            min_identity (int): The absolute minimum identity for a passing hit.
            max_identity (int): The absolute maximum identity for a passing hit.
            msf (bool): Whether or not to create an msf version of the MUSCLE alignment.
            fasta (bool): Whether or not to create an fasta version of the MUSCLE alignment.
        """
        protein_list_fn = os.path.join(self.protein_list_path, protein_list_fn)
        if not os.path.isfile(protein_list_fn):
            raise ValueError('Protein list file not cannot be found at specified location:\n{}'.format(protein_list_fn))
        self.protein_data = import_protein_list(protein_list_fn=os.path.join(self.protein_list_path, protein_list_fn))
        for protein_id in self.protein_data:
            self.protein_data[protein_id]['PDB'] = download_pdb(pdb_path=self.pdb_path, protein_id=protein_id)
            curr_seq, curr_len, curr_seq_fn = parse_query_sequence(protein_id=protein_id,
                                                                   chain_id=self.protein_data[protein_id]['Chain'],
                                                                   sequence_path=self.sequence_path,
                                                                   pdb_fn=self.protein_data[protein_id]['PDB'])
            self.protein_data[protein_id]['Sequence'] = curr_seq
            self.protein_data[protein_id]['Length'] = curr_len
            self.protein_data[protein_id]['Seq_Fasta'] = curr_seq_fn
            curr_hit_count, curr_blast_fn = blast_query_sequence(
                protein_id=protein_id, blast_path=self.blast_path, sequence_fn=curr_seq_fn, num_threads=num_threads,
                max_target_seqs=max_target_seqs, database=database, remote=remote)
            self.protein_data[protein_id]['BLAST_HITS'] = curr_hit_count
            self.protein_data[protein_id]['BLAST'] = curr_blast_fn
            curr_filter_count, curr_filter_fn = filter_blast_sequences(
                protein_id=protein_id, filter_path=self.filtered_blast_path, blast_fn=curr_blast_fn, query_seq=curr_seq,
                e_value_threshold=e_value_threshold, min_fraction=min_fraction, min_identity=min_identity,
                max_identity=max_identity)
            self.protein_data[protein_id]['Filter_Count'] = curr_filter_count
            self.protein_data[protein_id]['Filtered_BLAST'] = curr_filter_fn
            self._align_sequences(protein_id=protein_id, msf=msf, fasta=fasta)

    # def generate_protein_data(self, protein_id):

    def _align_sequences(self, protein_id, msf=True, fasta=True, source='Pileup_File'):
        """
        Align Sequences

        This method uses CLUSTALW to align the query sequence and all of the hits from BLAST which passed the filtering
        process by default the alignment is performed once, to produce a fasta alignment file, and then converted to
        produce the msf alignment file, however either of these options can be turned off.

        Args:
            protein_id (str): Four letter code for the PDB id for which an alignment should be performed.
            msf (bool): Whether or not to create an msf version of the MUSCLE alignment.
            fasta (bool): Whether or not to create an fasta version of the MUSCLE alignment.
        Returns:
            str: The path to the msf alignment produced by this method (None if msf=False).
            str: The path to the fasta alignment produced by this method (None if fa=False).
        """
        clustalw_path = os.environ.get('CLUSTALW_PATH')
        if self.protein_data[protein_id][source] is None:
            raise ValueError('Attempting to perform alignment before pileup file was generated.')
        alignment_path = os.path.join(self.input_path, 'Alignments')
        if not os.path.isdir(alignment_path):
            os.makedirs(alignment_path)
        fa_fn = None
        if fasta:
            fa_fn = os.path.join(alignment_path, '{}.fasta'.format(protein_id))
            if not os.path.isfile(fa_fn):
                fa_cline = ClustalwCommandline(clustalw_path, infile=self.protein_data[protein_id][source],
                                               align=True, quicktree=True, outfile=fa_fn, output='FASTA')
                print(fa_cline)
                stdout, stderr = fa_cline()
                print(stdout)
                print(stderr)
        msf_fn = None
        if msf:
            msf_fn = os.path.join(alignment_path, '{}.msf'.format(protein_id))
            if not os.path.isfile(msf_fn):
                if fasta:
                    msf_cline = ClustalwCommandline(clustalw_path, infile=fa_fn, convert=True, outfile=msf_fn,
                                                    output='GCG')
                else:
                    msf_cline = ClustalwCommandline(clustalw_path, infile=self.protein_data[protein_id][source],
                                               align=True, quicktree=True, outfile=msf_fn, output='GCG')
                print(msf_cline)
                stdout, stderr = msf_cline()
                print(stdout)
                print(stderr)
        self.protein_data[protein_id]['MSF_File'] = msf_fn
        self.protein_data[protein_id]['FA_File'] = fa_fn
        return msf_fn, fa_fn

    def _identity_filter(self, protein_id, max_identity=0.98):
        if self.protein_data[protein_id]['FA_File'] is None:
            raise ValueError('Attempting to refine alignment before an initial alignment has been generated')
        identity_filtered_path = os.path.join(self.input_path, 'Identity_Filtered')
        if not os.path.isdir(identity_filtered_path):
            os.makedirs(identity_filtered_path)
        calculator = AlignmentDistanceCalculator()
        alignment = SeqAlignment(file_name=self.protein_data[protein_id]['FA_File'], query_id=protein_id)
        alignment.import_alignment()
        distance_matrix = triu(np.array(calculator.get_distance(alignment.alignment)), k=1)
        to_keep = set()
        to_remove = set()
        query_seq_pos = alignment.seq_order.index(protein_id)
        to_keep.add(query_seq_pos)
        for i in range(alignment.size):
            if (i in to_remove) or (i in to_keep):
                continue
            row_ids = distance_matrix[i, :]
            above_max_id = row_ids > max_identity
            positions_to_remove = set(nonzero(above_max_id)[0])
            if not positions_to_remove.isdisjoint(to_keep):
                to_remove.add(i)
            else:
                to_keep.add(i)
                to_remove.update(positions_to_remove)
        filtering_count = (len(to_keep) + len(to_remove))
        if alignment.size != filtering_count:
            raise ValueError('Identity filtering does not match alignment size {} != {} = {} + {}'.format(
                alignment.size, filtering_count, len(to_keep), len(to_remove)))
        filtered_alignment = alignment.generate_sub_alignment([alignment.seq_order[x] for x in to_keep])
        identity_filtered_fn = os.path.join(identity_filtered_path, '{}.fasta'.format(protein_id))
        filtered_alignment.write_out_alignment(file_name=identity_filtered_fn)
        self.protein_data[protein_id]['Identity_Filtered_FA'] = identity_filtered_fn
        return identity_filtered_fn


def import_protein_list(protein_list_fn):
    """
    Import protein list

    This function opens the list file found at protein_list_fn and parses in the PDB ids and the chain of interest
    specified there.

    Args:
        protein_list_fn (str): The name of the file where the list of PDB ids and their chain of interest (five letter
        codes) of which the data set consists can be found. Each id is expected to be on its own line.
    Returns:
        dict: A dictionary where each key is a single PDB id parsed from the specified list file and each value is
        a dictionary with the first key and value being the specified chain of interest, and will be filled as the
        data set is constructed.
    """
    protein_list_fn = os.path.join(protein_list_fn)
    protein_list = {}
    pdb_id_pattern = compile(r'^([0-9][a-z0-9]{3})([A-Z])$')
    with open(protein_list_fn, mode='rb') as protein_list_handle:
        for line in protein_list_handle:
            pdb_id_match = pdb_id_pattern.match(line.strip())
            protein_list[pdb_id_match.group(1)] = {'Chain': pdb_id_match.group(2)}
    return protein_list


def download_pdb(pdb_path, protein_id):
    """
    Download PDB

    This function downloads the PDB structure file for the given PDB id provided. The file is stored in a file within a
    sub directory of the provided pdb_path named <pdb_path>/{middle two characters of the PDB id}/pdb{pdb id}.ent

    Args:
        pdb_path (str): The location at which PDB structures should be saved.
        protein_id (str): Four letter code for a PDB id to be downloaded.
    Returns:
        str: The path to the PDB file downloaded.
    """
    if not os.path.isdir(pdb_path):
        os.makedirs(pdb_path)
    pdb_list = PDBList(server='ftp://ftp.wwpdb.org', pdb=pdb_path)
    pdb_file = pdb_list.retrieve_pdb_file(pdb_code=protein_id, file_format='pdb')
    return pdb_file


def parse_query_sequence(protein_id, chain_id, sequence_path, pdb_fn):
    """
    Parse Query Sequence

    This function opens a downloaded PDB file for a given protein id (for which download_pdb has already been
    called) and extracts the sequence of the specified chain_id for the structure. The parsed sequence is given in
    single letter amino acid codes. The sequence is saved to a file in sequence_path and is given a file name with the
    pattern {protein id}.fasta.

    Args:
        protein_id (str): Four letter code for a PDB id whose sequence should be parsed.
        chain_id (str/char): A single letter code for the chain to be extracted.
        sequence_path (str): The path to a directory where the sequence data can be written in fasta format.
        pdb_fn (str): The full path to the PDB from which the sequence should be extracted.
    Returns:
        str: The sequence parsed from the PDB file of the specified protein id.
        int: The length of the parsed sequence.
        str: The file path to the fasta file where the sequence has been written.
    """
    if not os.path.isdir(sequence_path):
        os.makedirs(sequence_path)
    protein_fasta_fn = os.path.join(sequence_path, '{}.fasta'.format(protein_id))
    if os.path.isfile(protein_fasta_fn):
        with open(protein_fasta_fn, 'rb') as protein_fasta_handle:
            seq_iter = parse(handle=protein_fasta_handle, format='fasta')
            sequence = seq_iter.next()
            sequence.alphabet = ExtendedIUPACProtein
    else:
        parser = PDBParser(PERMISSIVE=1)  # corrective
        structure = parser.get_structure(protein_id, pdb_fn)
        model = structure[0]
        chain = model[chain_id]
        sequence = []
        for residue in chain:
            if is_aa(residue.get_resname(), standard=True):
                res_name = three_to_one(residue.get_resname())
                sequence.append(res_name)
        sequence = SeqRecord(Seq(''.join(sequence), alphabet=ExtendedIUPACProtein), id=protein_id,
                             description='Target Query')
        seq_records = [sequence]
        with open(protein_fasta_fn, 'wb') as protein_fasta_handle:
            write(sequences=seq_records, handle=protein_fasta_handle, format='fasta')
    return sequence, len(sequence), protein_fasta_fn


def blast_query_sequence(protein_id, blast_path, sequence_fn, evalue=0.05, num_threads=1, max_target_seqs=20000,
                         database='nr', remote=False):
    """
    BlAST Query Sequence

    This function uses a local instance of the BLAST tool and the uniref 90 database to search for homologs and
    orthologs of the specified protein. The blast results are stored in a subdirectory of the input_path named BLAST
    with a file name following the pattern {protein id}.xml. This method assumes that _parse_query_sequence has
    already been performed for the specified protein id.

    Args:
        protein_id (str): Four letter code for the PDB id whose sequence should be searched using BLAST.
        blast_path (str): The location where BLAST output can be written.
        sequence_fn (str): The full path to a fasta formatted file which can be used as input for BLAST.
        evalue (float): The E-value upper limit for BLAST hits
        num_threads (int): The number of threads to use when performing the BLAST search.
        max_target_seqs (int): The maximum number of hits to look for in the BLAST database.
        database (str): The name of the database used to search for paralogs, homologs, and orthologs.
        remote (bool): Whether to perform the call to blastp remotely or not (in this case num_threads is ignored).
    Returns:
        int: The number of hits returned by BLAST
        str: The path to the xml file storing the BLAST output.
    """
    if not os.path.isdir(blast_path):
        os.makedirs(blast_path)
    blast_fn = os.path.join(blast_path, '{}.xml'.format(protein_id))
    if not os.path.isfile(blast_fn):
        if remote:
            blastp_cline = NcbiblastpCommandline(cmd=os.path.join(os.environ.get('BLAST_PATH'), 'blastp'),
                                                 out=blast_fn, query=sequence_fn, outfmt=5, remote=True, ungapped=False,
                                                 max_target_seqs=max_target_seqs, evalue=evalue,
                                                 db=os.path.join(os.environ.get('BLAST_DB_PATH'), database))
        else:
            blastp_cline = NcbiblastpCommandline(cmd=os.path.join(os.environ.get('BLAST_PATH'), 'blastp'),
                                                 out=blast_fn, query=sequence_fn, outfmt=5, remote=False,
                                                 ungapped=False, num_threads=num_threads, evalue=evalue,
                                                 max_target_seqs=max_target_seqs,
                                                 db=os.path.join(os.environ.get('BLAST_DB_PATH'), database))
        print(blastp_cline)
        stdout, stderr = blastp_cline()
        print(stdout)
        print(stderr)
    with open(blast_fn, 'rb') as blast_handle:
        blast_record = NCBIXML.read(blast_handle)
        hit_count = len(blast_record.alignments)
    return hit_count, blast_fn


def filter_blast_sequences(protein_id, filter_path, blast_fn, query_seq, e_value_threshold=0.05, min_fraction=0.7,
                           min_identity=40, max_identity=98):
    """
    Restrict Sequences

    This method reads in the sequences found in a BLAST search for a given protein id. The BLAST results are also
    filtered such that the e-value must be less than or equal to the specified cutoff (this should be done in the
    blast_query_method already but it is checked here for completeness an in case a BLAST query is performed outside of
    this pipeline but used to generate a data set).It also filters sequences ensuring that they are all within the
    min_fraction length, i.e. if you divide a hit's sequence length by the query sequence length and vice versa, the
    smaller value must be greater than min_fraction. Finally, the method filters based on sequence identity. It tests
    the identity of a sequence and if it is within the range
    covered by min_identity and max_identity it passes.

    Args:
        protein_id (str): Four letter code for the PDB id whose BLAST search results should be filtered.
        filter_path (str): Directory where filtered sequences can be written in fasta format.
        blast_fn (str): The full path to the BLAST xml which should be filtered.
        query_seq (str): The query sequence, needed for filtering and final output.
        e_value_threshold (float): The maximum e-value for a passing hit.
        min_fraction (float): The minimum fraction of the query sequence length for a passing hit.
        min_identity (int): The absolute minimum identity for a passing hit.
        max_identity (int): The absolute maximum identity for a passing hit.
    Returns:
        int: The number of sequences passing the filters.
        str: The file path to the list of sequences writen out after filtering.
    """
    if not os.path.isdir(filter_path):
        os.makedirs(filter_path)
    pileup_fn = os.path.join(filter_path, '{}.fasta'.format(protein_id))
    sequences = []
    if os.path.isfile(pileup_fn):
        hsp_data_pattern = compile(r'^.*\sHSP_identity=(\d+)\sHSP_alignment_length=(\d+)\sFraction_length=([0-9]+[.][0-9]+).*$')
        with open(pileup_fn, 'rb') as pileup_handle:
            fasta_iter = parse(handle=pileup_handle, format='fasta')
            for seq_record in fasta_iter:
                if seq_record.description.endswith('Target Query'):  # Skip the target sequence in the pileup.
                    continue
                hsp_data_match = hsp_data_pattern.match(seq_record.description)
                subject_fraction = float(hsp_data_match.group(3))
                if subject_fraction < min_fraction:
                    raise ValueError('Sequences in the filtered fasta do not meet the length fraction requirement.\n'
                                     '{}: Fraction={}'.format(seq_record.id, subject_fraction))
                seq_id = int(hsp_data_match.group(1))
                seq_len = int(hsp_data_match.group(2))
                identity = seq_id / float(seq_len)
                if seq_id < min_identity or seq_id > max_identity:
                    raise ValueError('Sequences in the filtered fasta do not met the identity requirement.\n'
                                     '{}: Identity={}'.format(seq_record.id, identity))
                seq_record.alphabet = ExtendedIUPACProtein
                sequences.append(seq_record)
    else:
        with open(blast_fn, 'rb') as blast_handle:
            blast_record = NCBIXML.read(blast_handle)
            for alignment in blast_record.alignments:
                aln_seq_record = None
                aln_identity = None
                for hsp in alignment.hsps:
                    if hsp.expect <= e_value_threshold:  # Should already be controlled by BLAST e-value
                        subject_length = len(hsp.sbjct.replace('-', ''))
                        query_length = len(query_seq)
                        subject_fraction = min(subject_length / float(query_length),
                                               query_length / float(subject_length))
                        if min_fraction <= subject_fraction:
                            identity = hsp.identities / float(hsp.align_length)
                            if min_identity <= identity and identity <= max_identity:
                                if identity > aln_identity:
                                    new_description = '{} HSP_identity={} HSP_alignment_length={} Fraction_length={}'.format(
                                        alignment.hit_def, hsp.identities, hsp.align_length, subject_fraction)
                                    aln_seq_record = SeqRecord(Seq(hsp.sbjct, alphabet=ExtendedIUPACProtein),
                                                               id=alignment.hit_id, name=alignment.title,
                                                               description=new_description)
                                    aln_identity = identity
                if aln_seq_record:
                    sequences.append(aln_seq_record)
    count = len(sequences)
    if not os.path.isfile(pileup_fn):
        # Add query sequence so that this file can be fed directly to the alignment method.
        final_sequences = [query_seq] + sequences
        with open(pileup_fn, 'wb') as pileup_handle:
            write(sequences=final_sequences, handle=pileup_handle, format='fasta')
    return count, pileup_fn


def determine_identity_bin(identity_count, length, interval, abs_max_identity, abs_min_identity, min_identity,
                           identity_bins):
    """
    Determine Identity Bin

    This method determines which identity bin a sequence belongs in based on the settings used for filtering
    sequences in the _restrict_sequences function.

    Args:
        identity_count (int): The number of positions which match between the query and the BLAST hit.
        length (int): The number of positions in the aligned sequence (including gaps).
        interval (int): The interval on which to define bins between min_identity and abs_min_identity in case
        sufficient sequences are not found at min_identity.
        abs_max_identity (int): The absolute maximum identity for a passing hit.
        abs_min_identity (int): The absolute minimum identity for a passing hit.
        min_identity (int): The preferred minimum identity for a passing hit.
        identity_bins (set): All of the possible identity bin options.
    Returns:
        float: The identity bin in which the sequence belongs.
    """
    similarity = (identity_count / float(length)) * 100
    similarity_int = floor(similarity)
    similarity_bin = similarity_int - (similarity_int % interval)
    final_bin = None
    if abs_max_identity >= similarity_bin and similarity_bin >= abs_min_identity:
        if similarity_bin >= min_identity:
            final_bin = min_identity
        elif similarity_bin not in identity_bins and similarity_bin >= abs_min_identity:
            final_bin = abs_min_identity
        else:
            final_bin = similarity_bin
    return final_bin


def batch_iterator(iterator, batch_size):
    """
    Batch Iterator

    This can be used on any iterator, for example to batch up SeqRecord objects from Bio.SeqIO.parse(...), or to batch
    Alignment objects from Bio.AlignIO.parse(...), or simply lines from a file handle. It is a generator function, and
    it returns lists of the entries from the supplied iterator.  Each list will have
    batch_size entries, although the final list may be shorter.

    Args:
        iterator (iterable): An iterable object, in this case a parser from the Bio package is the intended input.
        batch_size (int): The maximum number of entries to return for each batch (the final batch from the iterator may
        be smaller).
    Returns:
        list: A list of length batch_size (unless it is the last batch in the iterator in which case it may be fewer) of
        entries from the provided iterator.

    Scribed from Biopython (https://biopython.org/wiki/Split_large_file)
    """
    entry = True  # Make sure we loop once
    while entry:
        batch = []
        while len(batch) < batch_size:
            try:
                entry = iterator.next()
            except StopIteration:
                entry = None
            if entry is None:
                # End of file
                break
            batch.append(entry)
        if batch:
            yield batch


def filter_uniref_fasta(in_path, out_path):
    """
    Filter Uniref Fasta

    This function can be used to filter the fasta files provided by Uniref to remove the low quality sequences and
    sequences which represent protein fragments.

    Args:
        in_path (str): The path to the fasta file to filter (should be provided by Uniref so that the expected patterns
        can be found).
        out_path (str): The path to which the filtered fasta should be written.
    """
    sequences = []
    fragment_pattern = compile(r'^.*(\(Fragment\)).*$')
    low_quality_pattern = compile(r'^.*(LOW QUALITY).*$')
    record_iter = parse(open(in_path), "fasta")
    for i, batch in enumerate(batch_iterator(record_iter, 10000)):
        print('Batch: {}'.format(i))
        for seq_record in batch:
            fragment_check = fragment_pattern.search(seq_record.description)
            low_quality_check = low_quality_pattern.search(seq_record.description)
            if fragment_check or low_quality_check:
                continue
            sequences.append(seq_record)
            if len(sequences) == 1000:
                with open(out_path, 'ab') as out_handle:
                    write(sequences=sequences, handle=out_handle, format='fasta')
                sequences = []


def characterize_alignment(file_name, query_id, abs_max_identity=95, abs_min_identity=30, min_identity=75, interval=5):
    """
    Characterize Alignment

    This function seqs to characterize a given fasta formatted alignment in the same way which the DataSetGenerator uses
    when filtering sequences returned by the BLAST search.

    Args:
        file_name (str or path): The path to the file from which the alignment can be parsed. If a relative path is
        used (i.e. the ".." prefix), python's path library will be used to attempt to define the full path.
        query_id (str): The sequence identifier of interest.
        abs_max_identity (int): The absolute maximum identity for a passing hit.
        abs_min_identity (int): The absolute minimum identity for a passing hit.
        min_identity (int): The preferred minimum identity for a passing hit.
        interval (int): The interval on which to define bins between min_identity and abs_min_identity in case
        sufficient sequences are not found at min_identity.
    Returns:
         float: The lowest fraction sequence length (expressed as a decimal) as compared to the query sequence.
         float: The highest fraction sequence length (expressed as a decimal) as compared to the query sequence.
         set: All fraction lengths observed in the provided alignment.
         dict: A dictionary mapping the identity bins, created by the provided abs_max_identity, abs_min_identity,
         min_identity, and interval values, to lists of sequence identifiers falling in that bin.
         dict: A dictionary mapping identity values, falling outside of the computed bins, to sequence identifiers
         having those identities when compared to the query sequence.
    """
    aln = SeqAlignment(file_name=file_name, query_id=query_id)
    aln.import_alignment()
    query_pos = aln.seq_order.index(query_id)
    full_query = str(aln.alignment[query_pos].seq)
    aligned_length = len(full_query)
    max_fraction = 0.0
    min_fraction = 1.0
    seq_fractions = []
    identity_bins = list(range(abs_min_identity, min_identity, interval))
    sequences = {x: [] for x in identity_bins}
    out_of_range = {}
    if min_identity not in sequences:
        identity_bins.append(min_identity)
        sequences[min_identity] = []
    for i in range(aln.size):
        if aln.seq_order[i] == query_id:
            continue
        full_seq = str(aln.alignment[i].seq)
        ungapped_seq = len(full_seq.replace('-', ''))
        subject_fraction = ungapped_seq / float(len(str(aln.query_sequence).replace('-', '')))
        if subject_fraction > max_fraction:
            max_fraction = subject_fraction
        if subject_fraction < min_fraction:
            min_fraction = subject_fraction
        seq_fractions.append(subject_fraction)
        id_count = 0
        for j in range(aligned_length):
            if full_query[j] == full_seq[j]:
                id_count += 1
        similarity_bin = determine_identity_bin(identity_count=id_count, length=aligned_length, interval=interval,
                                                abs_max_identity=abs_max_identity, abs_min_identity=abs_min_identity,
                                                min_identity=min_identity, identity_bins=set(identity_bins))
        if similarity_bin is None:
            identity = id_count / float(aligned_length)
            if identity not in out_of_range:
                out_of_range[identity] = []
            out_of_range[identity].append(aln.seq_order[i])
        else:
            sequences[similarity_bin].append(aln.seq_order[i])
    return min_fraction, max_fraction, seq_fractions, sequences, out_of_range


def parse_arguments():
    """
    parse arguments

    This method provides a nice interface for parsing command line arguments and includes help functionality.

    Returns:
        dict. A dictionary containing the arguments parsed from the command line and their arguments.
    """
    # Create input parser
    parser = argparse.ArgumentParser(description='Process DataSetGenerator command line arguments.')
    # Arguments for creating BLAST database input from a uniref fasta
    parser.add_argument('--custom_uniref', default=False, action='store_true',
                        help='Set this argument  to create a new fasta file as input to the makeblastdb tool.')
    parser.add_argument('--original_uniref_fasta', type=str, nargs='?',
                        help='The fasta to be filtered for the makeblastdb tool')
    parser.add_argument('--filtered_uniref_fasta', type=str, nargs='?',
                        help='The path where the filtered uniref fasta file should be saved.')
    # Arguments for characterizing an existing alignment
    parser.add_argument('--characterize_alignment', default=False, action='store_true',
                        help='Set this argument  to generate statistics on an alignment.')
    parser.add_argument('--file_name', type=str, nargs='?',
                        help="The path to the file from which the alignment can be parsed. If a relative path is used "
                             "(i.e. the '..' prefix), python's path library will be used to attempt to define the full "
                             "path.")
    parser.add_argument('--query_id', type=str, nargs='?',
                        help='The sequence identifier of interest.')
    # Arguments for creating a data set using the DataSet generator
    parser.add_argument('--create_data_set', default=False, action='store_true',
                        help='Set this argument to create a data set, i.e. download and generate all input files.')
    parser.add_argument('--input_dir', type=str, nargs='?', help='The path to the directory where the data for this '
                                                                 'data set should be stored. It is expected to contain '
                                                                 'at least one directory name ProteinLists which should'
                                                                 ' contain files where each line denotes a separate PDB'
                                                                 ' id (four letter code only).')
    parser.add_argument('--protein_list_fn', type=str, nargs='?',
                        help='The name of the file where the list of PDB ids (four letter codes) of which the data set '
                             'consists can be found. Each id is expected to be on its own line.')
    parser.add_argument('--num_threads', type=int, default=1, nargs=1,
                        help='The number of threads to use when performing the BLAST search.')
    parser.add_argument('--max_target_seqs', type=int, default=20000, nargs=1,
                        help='The maximum number of hits to look for in the BLAST database.')
    parser.add_argument('--e_value_threshold', type=float, default=0.05, nargs=1,
                        help='The maximum e-value for a passing hit.')
    parser.add_argument('--min_fraction', type=float, default=0.7, nargs=1,
                        help='The minimum fraction of the query sequence length for a passing hit.')
    parser.add_argument('--max_fraction', type=float, default=1.5, nargs=1,
                        help='The maximum fraction of the query sequence length for a passing hit.')
    parser.add_argument('--min_identity', type=int, default=75, nargs=1,
                        help='The preferred minimum identity for a passing hit.')
    parser.add_argument('--abs_max_identity', type=int, default=95, nargs=1,
                        help='The absolute maximum identity for a passing hit.')
    parser.add_argument('--abs_min_identity', type=int, default=30, nargs=1,
                        help='The absolute minimum identity for a passing hit.')
    parser.add_argument('--interval', type=int, default=5, nargs=1,
                        help='The interval on which to define bins between min_identity and abs_min_identity in case '
                             'sufficient sequences are not found at min_identity.')
    parser.add_argument('--ignore_filter_size', type=bool, default=False, nargs=1,
                        help='Whether or not to ignore the 125 sequence requirement before writing the filtered '
                             'sequences to file.')
    parser.add_argument('--msf', type=bool, default=True, nargs=1,
                        help='Whether or not to create an msf version of the MUSCLE alignment.')
    parser.add_argument('--fasta', type=bool, default=True, nargs=1,
                        help='Whether or not to create an fasta version of the MUSCLE alignment.')
    # Clean command line input
    arguments = parser.parse_args()
    arguments = vars(arguments)
    processor_count = cpu_count()
    if arguments['num_threads'] > processor_count:
        arguments['num_threads'] = processor_count
    if arguments['custom_uniref']:
        if (not 'original_uniref_fasta' in arguments) or (not 'filtered_uniref_fasta'):
            raise ValueError('When custom_uniref is selected original_uniref_fasta and filtered_uniref_fasta must be specfied.')
    if arguments['characterize_alignment']:
        if (not 'file_name' in arguments) or (not 'query_id' in arguments):
            raise ValueError('When characterize_alignment is selected file_name and query_id must be specified.')
    if arguments['create_data_set']:
        if (not 'input_dir' in arguments) or (not 'protein_list_fn' in arguments):
            raise ValueError('When create_data_set is selected input_dir and protein_list_fn must be specified.')
    return arguments


if __name__ == "__main__":
    args = parse_arguments()
    if args['custom_uniref']:
        filter_uniref_fasta(in_path=args['original_uniref_fasta'], out_path=args['filtered_uniref_fasta'])
    if args['characterize_alignment']:
        res = characterize_alignment(file_name=args['file_name'], query_id=args['query_id'],
                                     abs_max_identity=args['abs_max_identity'],
                                     abs_min_identity=args['abs_min_identity'], min_identity=args['min_identity'],
                                     interval=args['interval'])
        print('Sequence Fraction:\n\tMinimum:\t{}\n\tMaximum:\t{}\n\tAverage:\t{}'.format(res[0], res[1], mean(res[2])))
        print('\nSequence Identities:\n')
        for id_bin in sorted(res[3].keys()):
            print('\tBin_{}:\t{} Sequences'.format(id_bin, len(res[3][id_bin])))
        print('\nSequence Identities Outside Expected Range:\n')
        for id_bin in sorted(res[4].keys()):
            print('\tBin_{}:\t{} Sequences'.format(id_bin, len(res[4][id_bin])))
    if args['create_data_set']:
        generator = DataSetGenerator(protein_list=args['protein_list_fn'], input_path=args['input_dir'])
        generator.build_dataset(num_threads=args['num_threads'], max_target_seqs=args['max_target_seqs'],
                                e_value_threshold=args['e_value_threshold'], min_fraction=args['min_fraction'],
                                max_fraction=args['max_fraction'], min_identity=args['min_identity'],
                                abs_max_identity=args['abs_max_identity'], abs_min_identity=args['abs_min_identity'],
                                interval=args['interval'], ignore_filter_size=args['ignore_filter_size'],
                                msf=args['msf'], fasta=args['fasta'])
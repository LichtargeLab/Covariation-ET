"""
Created on May 23, 2019

@author: Daniel Konecki
"""
import os
import argparse
import numpy as np
import pandas as pd
from time import time
from re import compile
from numpy import floor, triu, nonzero
from multiprocessing import cpu_count, Pool, Lock
from Bio.Seq import Seq
from Bio.Blast import NCBIXML
from Bio.SeqIO import write, parse
from Bio.SeqRecord import SeqRecord
from Bio.PDB.PDBList import PDBList
from Bio.Application import ApplicationError
from Bio.Align.Applications import ClustalwCommandline
from Bio.Blast.Applications import NcbiblastpCommandline
from dotenv import find_dotenv, load_dotenv
from src.SupportingClasses.PDBReference import PDBReference
from src.SupportingClasses.SeqAlignment import SeqAlignment
from src.SupportingClasses.AlignmentDistanceCalculator import AlignmentDistanceCalculator
from src.SupportingClasses.EvolutionaryTraceAlphabet import FullIUPACProtein
try:
    dotenv_path = find_dotenv(raise_error_if_not_found=True)
except IOError:
    dotenv_path = find_dotenv(raise_error_if_not_found=True, usecwd=True)
load_dotenv(dotenv_path)


class DataSetGenerator(object):
    """
    This class is meant to automate data set generation and improve reproducibility of papers from our lab which rely on
    PDB structures and alignments generated by BLASTING the proteins represented in those structures.

    Attributes:
        input_path (str/path): The path to the directory where the data for this data set should be stored. It is
        expected to contain at least one directory name ProteinLists which should contain files where each line denotes
        a separate PDB id (four letter code only).
        pdb_path (str): The location at which PDB structures should be saved.
        sequence_path (str): The path to a directory where the sequence data can be written in fasta format.
        blast_path (str): The location where BLAST output can be written.
        filtered_blast_path (str): Directory where filtered sequences can be written in fasta format.
        alignment_path (str): The directory to which the initial alignments can be written.
        filtered_alignment_path (str): Path to a directory where the filtered alignment can be written.
        final_alignment_path (str): The directory to which the final alignments can be written.
        protein_data (dict): A dictionary to hold the protein data generated over the course of data set construction.
        It is initially None and is then built up such that it contains the PDB ids parsed in from the protein list
        as keys, referencing dictionaries with each proteins values.
    """

    def __init__(self, input_path):
        """
        Init

        This function overwrites the default init function for the DataSetGenerator class.

        Args:
            input_path (path): The path to the directory where the data for this data set should be stored. It is
            expected to contain at least one directory name ProteinLists which should contain files where each line
            denotes a separate PDB id (four letter code only).
        """
        if input_path is None:
            self.input_path = os.environ.get('INPUT_PATH')
        else:
            self.input_path = input_path
        self.protein_list_path = os.path.join(self.input_path, 'ProteinLists')
        self.pdb_path = os.path.join(self.input_path, 'PDB')
        if not os.path.isdir(self.pdb_path):
            os.makedirs(self.pdb_path)
        self.sequence_path = os.path.join(self.input_path, 'Sequences')
        if not os.path.isdir(self.sequence_path):
            os.makedirs(self.sequence_path)
        self.blast_path = os.path.join(self.input_path, 'BLAST')
        if not os.path.isdir(self.blast_path):
            os.makedirs(self.blast_path)
        self.filtered_blast_path = os.path.join(self.input_path, 'Filtered_BLAST')
        if not os.path.isdir(self.filtered_blast_path):
            os.makedirs(self.filtered_blast_path)
        self.alignment_path = os.path.join(self.input_path, 'Alignments')
        if not os.path.isdir(self.alignment_path):
            os.makedirs(self.alignment_path)
        self.filtered_alignment_path = os.path.join(self.input_path, 'Filtered_Alignment')
        if not os.path.isdir(self.filtered_alignment_path):
            os.makedirs(self.filtered_alignment_path)
        self.final_alignment_path = os.path.join(self.input_path, 'Final_Alignments')
        if not os.path.isdir(self.final_alignment_path):
            os.makedirs(self.final_alignment_path)
        self.protein_data = None

    def build_pdb_alignment_dataset(self, protein_list_fn, num_threads=1, max_target_seqs=2500, e_value_threshold=0.05,
                                    database='customuniref90.fasta', remote=False, min_fraction=0.7, min_identity=0.40,
                                    max_identity=0.98, msf=True, fasta=True, verbose=False):
        """
        Build Dataset

        This method builds a complete data set based on the protein id list specified in the constructor.

        Args:
            protein_list_fn (str): The name of the file where the list of PDB ids (five letter codes, first four
            characters are the PDB ID and the last character is the chain id) of which the data set consists can be
            found. Each id is expected to be on its own line.
            num_threads (int): The number of threads to use when performing the BLAST search.
            max_target_seqs (int): The maximum number of hits to look for in the BLAST database.
            e_value_threshold (float): The maximum e-value for a passing hit.
            database (str): The name of the database used to search for paralogs, homologs, and orthologs.
            remote (bool): Whether to perform the call to blastp remotely or not (in this case num_threads is ignored).
            min_fraction (float): The minimum fraction of the query sequence length for a passing hit.
            min_identity (float): The absolute minimum identity for a passing hit.
            max_identity (float): The absolute maximum identity for a passing hit.
            msf (bool): Whether or not to create an msf version of the MUSCLE alignment.
            fasta (bool): Whether or not to create an fasta version of the MUSCLE alignment.
            verbose (bool): Whether to write out information during processing.
        Return:
            pandas.DataFrame: Summary of sequence counts at the BLAST, filtered BLAST, and filtered alignment stages as
            well as the time it took to generate data for a specific sequence.
        """
        data_set_name = os.path.splitext(os.path.basename(protein_list_fn))[0]
        protein_list_fn = os.path.join(self.protein_list_path, protein_list_fn)
        if not os.path.isfile(protein_list_fn):
            raise ValueError('Protein list file not cannot be found at specified location:\n{}'.format(protein_list_fn))
        print('Importing protein list')
        self.protein_data = import_protein_list(protein_list_fn=os.path.join(self.protein_list_path, protein_list_fn))
        # Download the PDBs and parse out the query sequences.
        print('Downloading structures and parsing in query sequences')
        pool1 = Pool(num_threads, initializer=init_pdb_processing_pool, initargs=(self.pdb_path, self.sequence_path,
                                                                                  Lock(), verbose))
        res1 = pool1.map_async(pdb_processing, [(p_id, self.protein_data[p_id]['Chain']) for p_id in self.protein_data])
        pool1.close()
        pool1.join()
        res1 = res1.get()
        seqs_to_write = []
        for data1 in res1:
            self.protein_data[data1[0]].update(data1[1])
            if data1[1]['Sequence']:
                seqs_to_write.append(data1[1]['Sequence'])
        # Write out a fasta file containing all the query sequences for BLASTing
        all_seqs_fn = os.path.join(self.sequence_path, data_set_name + '.fasta')
        if not os.path.isfile(all_seqs_fn):
            with open(all_seqs_fn, 'w') as all_seqs_handle:
                write(seqs_to_write, all_seqs_handle, 'fasta')
        # BLAST all query sequences at once
        print('BLASTing query sequences')
        blast_fn, hits = blast_query_sequence(protein_id=data_set_name + '_All_Seqs', blast_path=self.blast_path,
                                              sequence_fn=all_seqs_fn, evalue=e_value_threshold,
                                              num_threads=num_threads, max_target_seqs=max_target_seqs,
                                              database=database, remote=remote)
        for p_id in hits:
            self.protein_data[p_id].update(hits[p_id])
            self.protein_data[p_id]['BLAST'] = blast_fn
        # Filter the BLAST hits for each query, align, filter again, and make final alignments.
        print('Filtering BLAST hits, aligning, filtering by identity, and re-aligning')
        pool2 = Pool(num_threads, initializer=init_filtering_and_alignment_pool,
                    initargs=(max_target_seqs, e_value_threshold, database, remote, min_fraction, min_identity,
                              max_identity, msf, fasta, blast_fn, self.filtered_blast_path, self.alignment_path,
                              self.filtered_alignment_path, self.final_alignment_path, verbose))
        res = pool2.map_async(filtering_and_alignment, [(p_id, self.protein_data[p_id]['Sequence'])
                                                        for p_id in self.protein_data])
        pool2.close()
        pool2.join()
        res = res.get()
        summary = {'Protein_ID': [], 'BLAST_Hits': [], 'Filtered_BLAST': [], 'Filtered_Alignment': [], 'Time': []}
        for data in res:
            # Add all the data generated (data[1]) to the protein data dict under the correct protein id (data[0])
            self.protein_data[data[0]].update(data[1])
            summary['Protein_ID'].append(data[0])
            summary['BLAST_Hits'].append(hits[data[0]]['BLAST_Hits'])
            summary['Filtered_BLAST'].append(data[1]['Filter_Count'])
            summary['Filtered_Alignment'].append(data[1]['Final_Count'])
            summary['Time'].append(data[2])
            if verbose:
                print('It took {} min to generate data for {}'.format(data[2], data[0]))
                print('\t{} Sequence Returned By Blast\n\t{} Sequences After Initial Filtering\n\t{} Sequences After '
                      'Identity Filtering'.format(hits[data[0]]['BLAST_Hits'], data[1]['Filter_Count'],
                                                  data[1]['Final_Count']))
        df = pd.DataFrame(summary)
        return df


def import_protein_list(protein_list_fn, verbose=False):
    """
    Import protein list

    This function opens the list file found at protein_list_fn and parses in the PDB ids and the chain of interest
    specified there.

    Args:
        protein_list_fn (str): The name of the file where the list of PDB ids and their chain of interest (five letter
        codes) of which the data set consists can be found. Each id is expected to be on its own line.
        verbose (bool): Whether or not to print out updates about the import
    Returns:
        dict: A dictionary where each key is a single PDB id parsed from the specified list file and each value is
        a dictionary with the first key and value being the specified chain of interest, and will be filled as the
        data set is constructed.
    """
    if verbose:
        print('Importing protein list from: {}'.format(protein_list_fn))
    protein_list_fn = os.path.join(protein_list_fn)
    protein_list = {}
    pdb_id_pattern = compile(r'^([0-9][a-z0-9]{3})([A-Z])$')
    with open(protein_list_fn, mode='r') as protein_list_handle:
        for line in protein_list_handle:
            pdb_id_match = pdb_id_pattern.match(line.strip())
            protein_list[pdb_id_match.group(1)] = {'Chain': pdb_id_match.group(2)}
    if verbose:
        print('Imported {} protein IDs'.format(len(protein_list)))
    return protein_list


def download_pdb(pdb_path, protein_id, verbose=False):
    """
    Download PDB

    This function downloads the PDB structure file for the given PDB id provided. The file is stored in a file within a
    sub directory of the provided pdb_path named <pdb_path>/{middle two characters of the PDB id}/pdb{pdb id}.ent. If
    the PDB structure is not available then an attempt is made to download it as an 'obsolete' PDB structure. If this is
    also unsuccessful, None is returned.

    Args:
        pdb_path (str): The location at which PDB structures should be saved.
        protein_id (str): Four letter code for a PDB id to be downloaded.
        verbose (bool): Whether to write out information during processing.
    Returns:
        str: The path to the PDB file downloaded.
    """
    if verbose:
        print('Downloading structure data for: {}'.format(protein_id))
    if not os.path.isdir(pdb_path):
        if verbose:
            print('Making dir: {}'.format(pdb_path))
        os.makedirs(pdb_path)
    pdb_list = PDBList(server='ftp://ftp.wwpdb.org', pdb=pdb_path, verbose=verbose)
    pdb_file = pdb_list.retrieve_pdb_file(pdb_code=protein_id, file_format='pdb')
    if not os.path.isfile(pdb_file):
        pdb_file = pdb_list.retrieve_pdb_file(pdb_code=protein_id, file_format='pdb', obsolete=True)
        if not os.path.isfile(pdb_file):
            pdb_file = None
            print('Structure does not exist in obsolete: {}'.format(protein_id))
    if verbose:
        print('Completed {} structure download'.format(protein_id))
    return pdb_file


def parse_query_sequence(protein_id, chain_id, sequence_path, pdb_fn, verbose=False):
    """
    Parse Query Sequence

    This function opens a downloaded PDB file for a given protein id (for which download_pdb has already been
    called) and extracts the sequence of the specified chain_id for the structure. The parsed sequence is given in
    single letter amino acid codes. The sequence is saved to a file in sequence_path and is given a file name with the
    pattern {protein id}.fasta. If the specified chain_id is not available for the structure the first chain
    (alphabetically) is used. The chain is returned as the last value so the user knows if the specified chain was used
    or not.

    Args:
        protein_id (str): Four letter code for a PDB id whose sequence should be parsed.
        chain_id (str/char): A single letter code for the chain to be extracted.
        sequence_path (str): The path to a directory where the sequence data can be written in fasta format.
        pdb_fn (str): The full path to the PDB from which the sequence should be extracted.
        verbose (bool): Whether or not to write out warnings from PDB parsing.
    Returns:
        str: The sequence parsed from the PDB file of the specified protein id.
        int: The length of the parsed sequence.
        str: The file path to the fasta file where the sequence has been written.
        str: The chain id for which the sequence was parsed.
    """
    if verbose:
        print('Parsing query sequence for: {}'.format(protein_id))
    if not os.path.isdir(sequence_path):
        if verbose:
            print('Making dir: {}'.format(sequence_path))
        os.makedirs(sequence_path)
    protein_fasta_fn = os.path.join(sequence_path, '{}.fasta'.format(protein_id))
    if os.path.isfile(protein_fasta_fn):
        if verbose:
            print('Loading query sequence from file: {}'.format(protein_fasta_fn))
        with open(protein_fasta_fn, 'r') as protein_fasta_handle:
            seq_iter = parse(handle=protein_fasta_handle, format='fasta')
            sequence = next(seq_iter)
            sequence.alphabet = FullIUPACProtein()
    else:
        if verbose:
            print('Parsing query sequence from file: {}'.format(pdb_fn))
        pdb_struct = PDBReference(pdb_file=pdb_fn)
        pdb_struct.import_pdb(structure_id=protein_id)
        try:
            seq = pdb_struct.seq[chain_id]
        except KeyError:
            chain_id = sorted(pdb_struct.seq.keys())
            seq = pdb_struct.seq[chain_id]
        sequence = SeqRecord(Seq(seq, alphabet=FullIUPACProtein()), id=protein_id, description='Target Query')
        seq_records = [sequence]
        with open(protein_fasta_fn, 'w') as protein_fasta_handle:
            write(sequences=seq_records, handle=protein_fasta_handle, format='fasta')
    if verbose:
        print('Parsed query sequence for: {}'.format(protein_id))
    return sequence, len(sequence), protein_fasta_fn, chain_id


def init_pdb_processing_pool(pdb_path, sequence_path, lock, verbose):
    """
    Init PDB Processing Pool

    This function initializes variables needed by all threads downloading PDBs and processing their query sequence.

    Args:
        pdb_path (str): The location at which PDB structures should be saved.
        sequence_path (str): The path to a directory where the sequence data can be written in fasta format.
        lock (multiprocessing.Lock): A lock to control filesystem access while downloading PDBs since many folders need
        to be checked for and created.
        verbose (bool): Whether or not to write out verbose output.
    """
    global pdb_dir
    pdb_dir = pdb_path
    global sequence_dir
    sequence_dir = sequence_path
    global fs_lock
    fs_lock = lock
    global verbose_out
    verbose_out = verbose


def pdb_processing(in_tuple):
    """
    PDB Processing

    This function servers to download a single PDB id's structure and parse out its query sequence.

    Args:
        in_tuple (tuple): A tuple containing the protein_id and chain_id for the desired protein structure to download
        and sequence to parse.
    Returns:
        str: The protein_id passed in, needed for indexing upon return
        dict: A dictionary containing data to be added to the protein_data field of the DataSetGenerator including the
        following keys and values:
            PDB (str): The full path to the PDB structure downloaded for this protein.
            Chain (str): The chain for which the sequence was parsed, should be the same as that in the protein list
            file, but may change if that chain is not available. In that case the first (alphabetical) chain is used.
            Sequence (Bio.SeqRecord.SeqRecord): A SeqRecord containing the protein_id and the parsed out sequence.
            Length (int): The length of the parsed sequence.
            Seq_Fasta (str): The full path to the fasta file containing just the query sequence for this protein.
    """
    protein_id = in_tuple[0]
    chain_id = in_tuple[1]
    fs_lock.acquire()
    pdb_fn = download_pdb(pdb_path=pdb_dir, protein_id=protein_id, verbose=verbose_out)
    fs_lock.release()
    if pdb_fn is None:
        seq, length, seq_fn = None, None, None
    else:
        seq, length, seq_fn, chain_id = parse_query_sequence(protein_id=protein_id, chain_id=chain_id,
                                                             sequence_path=sequence_dir, pdb_fn=pdb_fn,
                                                             verbose=verbose_out)
    data = {'PDB': pdb_fn, 'Chain': chain_id, 'Sequence': seq, 'Length': length, 'Seq_Fasta': seq_fn}
    return protein_id, data


def blast_query_sequence(protein_id, blast_path, sequence_fn, evalue=0.05, num_threads=1, max_target_seqs=20000,
                         database='nr', remote=False, verbose=False):
    """
    BlAST Query Sequence

    This function uses a local instance of the BLAST tool and the specified database to search for homologs and
    orthologs of the specified protein. The BLAST results are stored in a subdirectory of the input_path named BLAST
    with a file name following the pattern {protein id}.xml. This method assumes that _parse_query_sequence has
    already been performed for the specified protein id.

    Args:
        protein_id (str): Four letter code for the PDB id whose sequence should be searched using BLAST.
        blast_path (str): The location where BLAST output can be written.
        sequence_fn (str): The full path to a fasta formatted file which can be used as input for BLAST.
        evalue (float): The E-value upper limit for BLAST hits
        num_threads (int): The number of threads to use when performing the BLAST search.
        max_target_seqs (int): The maximum number of hits to look for in the BLAST database.
        database (str): The name of the database used to search for paralogs, homologs, and orthologs.
        remote (bool): Whether to perform the call to blastp remotely or not (in this case num_threads is ignored).
        verbose (bool): Whether to write out information during processing.
    Returns:
        str: The path to the xml file storing the BLAST output.
        dict: Mapping between protein_ids and a dictionary of the BLAST_Hit key and the number of hits returned.
    """
    if verbose:
        print('BLASTing query sequence for: {}'.format(protein_id))
    if not os.path.isdir(blast_path):
        if verbose:
            print('Making dir: {}'.format(blast_path))
        os.makedirs(blast_path)
    blast_fn = os.path.join(blast_path, '{}.xml'.format(protein_id))
    if not os.path.isfile(blast_fn):
        start = time()
        if remote:
            blastp_cline = NcbiblastpCommandline(cmd=os.path.join(os.environ.get('BLAST_PATH'), 'blastp'),
                                                 out=blast_fn, query=sequence_fn, outfmt=5, remote=True, ungapped=False,
                                                 max_target_seqs=max_target_seqs, evalue=evalue, db=database)
        else:
            blastp_cline = NcbiblastpCommandline(cmd=os.path.join(os.environ.get('BLAST_PATH'), 'blastp'),
                                                 out=blast_fn, query=sequence_fn, outfmt=5, remote=False,
                                                 ungapped=False, num_threads=num_threads, evalue=evalue,
                                                 max_target_seqs=max_target_seqs,
                                                 db=os.path.join(os.environ.get('BLAST_DB_PATH'), database))
        if verbose:
            print(blastp_cline)
        print(blastp_cline)
        stdout, stderr = blastp_cline()
        if verbose:
            print(stdout)
            print(stderr)
        end = time()
    else:
        start = end = time()
        if verbose:
            print('BLAST previously completed for: {}'.format(protein_id))
    hit_counts = {}
    with open(blast_fn, 'r') as blast_handle:
        pdb_id_pattern = compile(r'^([0-9][a-z0-9]{3}).*$')
        blast_iter = NCBIXML.parse(blast_handle)
        for blast_record in blast_iter:
            pdb_id_match = pdb_id_pattern.match(blast_record.query)
            hit_counts[pdb_id_match.group(1)] = {'BLAST_Hits': len(blast_record.alignments)}
    if verbose:
        print('BLAST for {} completed in {} mins'.format(protein_id, (end - start) / 60.0))
    return blast_fn, hit_counts


def filter_blast_sequences(protein_id, filter_path, blast_fn, query_seq, e_value_threshold=0.05, min_fraction=0.7,
                           min_identity=0.40, max_identity=0.98, alphabet=FullIUPACProtein(), verbose=False):
    """
    Restrict Sequences

    This method reads in the sequences found in a BLAST search for a given protein id. The BLAST results are also
    filtered such that the e-value must be less than or equal to the specified cutoff (this should be done in the
    blast_query_method already but it is checked here for completeness an in case a BLAST query is performed outside of
    this pipeline but used to generate a data set).It also filters sequences ensuring that they are all within the
    min_fraction length, i.e. if you divide a hit's sequence length by the query sequence length and vice versa, the
    smaller value must be greater than min_fraction. Finally, the method filters based on sequence identity. It tests
    the identity of a sequence and if it is within the range
    covered by min_identity and max_identity it passes.

    Args:
        protein_id (str): Four letter code for the PDB id whose BLAST search results should be filtered.
        filter_path (str): Directory where filtered sequences can be written in fasta format.
        blast_fn (str): The full path to the BLAST xml which should be filtered.
        query_seq (Bio.SeqRecord.SeqRecord): The query sequence, needed for filtering and final output.
        e_value_threshold (float): The maximum e-value for a passing hit.
        min_fraction (float): The minimum fraction of the query sequence length for a passing hit.
        min_identity (float): The absolute minimum identity for a passing hit.
        max_identity (float): The absolute maximum identity for a passing hit.
        alphabet (Bio.Alphabet.Alphavet): The alphabet to use when filtering BLAST sequences.
        verbose (bool): Whether to write out information during processing.
    Returns:
        int: The number of sequences passing the filters.
        str: The file path to the list of sequences writen out after filtering.
    """
    if not os.path.isdir(filter_path):
        os.makedirs(filter_path)
    pileup_fn = os.path.join(filter_path, '{}.fasta'.format(protein_id))
    sequences = []
    if os.path.isfile(pileup_fn):
        hsp_data_pattern = compile(r'^.*\sHSP_identity=(\d+)\sHSP_alignment_length=(\d+)\sFraction_length=([0-9]+[.][0-9]+).*$')
        with open(pileup_fn, 'r') as pileup_handle:
            fasta_iter = parse(handle=pileup_handle, format='fasta')
            for seq_record in fasta_iter:
                if seq_record.description.endswith('Target Query'):  # Add the target sequence without checking.
                    sequences.append(seq_record)
                    continue
                hsp_data_match = hsp_data_pattern.match(seq_record.description)
                subject_fraction = float(hsp_data_match.group(3))
                if subject_fraction < min_fraction:
                    raise ValueError('Sequences in the filtered fasta do not meet the length fraction requirement.\n'
                                     '{}: Fraction={}'.format(seq_record.id, subject_fraction))
                seq_id = int(hsp_data_match.group(1))
                seq_len = int(hsp_data_match.group(2))
                identity = seq_id / float(seq_len)
                if identity < min_identity or identity > max_identity:
                    raise ValueError('Sequences in the filtered fasta do not met the identity requirement.\n'
                                     '{}: Identity={}'.format(seq_record.id, identity))
                seq_record.alphabet = FullIUPACProtein()
                sequences.append(seq_record)
    else:
        description_pattern = '{} HSP_identity={} HSP_alignment_length={} Fraction_length={}'
        with open(blast_fn, 'r') as blast_handle:
            blast_iter = NCBIXML.parse(blast_handle)
            for blast_record in blast_iter:
                if not blast_record.query.startswith(protein_id):
                    continue
                for alignment in blast_record.alignments:
                    aln_seq_record = None
                    aln_identity = None
                    for hsp in alignment.hsps:
                        if hsp.expect <= e_value_threshold:  # Should already be controlled by BLAST e-value
                            subject_length = len(hsp.sbjct.replace('-', ''))
                            query_length = len(query_seq.seq)
                            subject_fraction = min(subject_length / float(query_length),
                                                   query_length / float(subject_length))
                            if min_fraction <= subject_fraction:
                                identity = hsp.identities / float(hsp.align_length)
                                if min_identity <= identity and identity <= max_identity:
                                    if (aln_identity is None) or (identity > aln_identity):
                                        new_description = description_pattern.format(alignment.hit_def, hsp.identities,
                                                                                     hsp.align_length, subject_fraction)
                                        aln_seq_record = SeqRecord(Seq(hsp.sbjct, alphabet=FullIUPACProtein()),
                                                                   id=alignment.hit_id, name=alignment.title,
                                                                   description=new_description)
                                        aln_identity = identity
                    if aln_seq_record:
                        sequences.append(aln_seq_record)
        # Add query sequence so that this file can be fed directly to the alignment method.
        sequences = [query_seq] + sequences
        with open(pileup_fn, 'w') as pileup_handle:
            write(sequences=sequences, handle=pileup_handle, format='fasta')
    count = len(sequences)
    return count, pileup_fn


def align_sequences(protein_id, alignment_path, pileup_fn, msf=True, fasta=True, verbose=False):
    """
    Align Sequences

    This method uses CLUSTALW to align the query sequence and all of the hits from BLAST which passed the filtering
    process by default the alignment is performed once, to produce a fasta alignment file, and then converted to
    produce the msf alignment file, however either of these options can be turned off.

    Args:
        protein_id (str): Four letter code for the PDB id for which an alignment should be performed.
        alignment_path (str): The directory to which the alignments can be written.
        pileup_fn (str): The full path to the file with the sequences which should be aligned.
        msf (bool): Whether or not to create an msf version of the MUSCLE alignment.
        fasta (bool): Whether or not to create an fasta version of the MUSCLE alignment.
        verbose (bool): Whether to write out information during processing.
    Returns:
        str: The path to the fasta alignment produced by this method (None if fa=False).
        str: The path to the msf alignment produced by this method (None if msf=False).
    """
    clustalw_path = os.environ.get('CLUSTALW_PATH')
    if not os.path.isdir(alignment_path):
        os.makedirs(alignment_path)
    if pileup_fn is None:
        raise ValueError('Pileup filename provided is None, pileup file is required to perform an alignment.')
    fa_fn = None
    if fasta:
        fa_fn = os.path.join(alignment_path, '{}.fasta'.format(protein_id))
        if not os.path.isfile(fa_fn):
            fa_cline = ClustalwCommandline(clustalw_path, infile=pileup_fn, align=True, quicktree=True, outfile=fa_fn,
                                           output='FASTA')
            if verbose:
                print(fa_cline)
            try:
                stdout, stderr = fa_cline()
                if verbose:
                    print(stdout)
                    print(stderr)
            except ApplicationError:
                fa_fn = None
    msf_fn = None
    if msf:
        msf_fn = os.path.join(alignment_path, '{}.msf'.format(protein_id))
        if not os.path.isfile(msf_fn):
            if fasta:
                msf_cline = ClustalwCommandline(clustalw_path, infile=fa_fn, convert=True, outfile=msf_fn, output='GCG')
            else:
                msf_cline = ClustalwCommandline(clustalw_path, infile=pileup_fn, align=True, quicktree=True,
                                                outfile=msf_fn, output='GCG')
            if verbose:
                print(msf_cline)
            try:
                stdout, stderr = msf_cline()
                if verbose:
                    print(stdout)
                    print(stderr)
            except ApplicationError:
                msf_fn = None
    return msf_fn, fa_fn


def identity_filter(protein_id, filter_path, alignment_fn, max_identity=0.98):
    """
    Identity Filter

    This method imports a previously generated alignment and then filters it so that no sequences have an identity
    greater than max_identity. If two or more sequences have an identity greater than max_identity the first sequence in
    that group is kept an all others are discarded (unless one of the sequences is the query sequence, in which the
    query sequence is kept and all other sequences in the group are discarded).

    Args:
        protein_id (str): The PDB id for the protein whose alignment is being filtered.
        filter_path (str): Path to a directory where the filtered alignment can be written.
        alignment_fn (str): Full path to the alignment file to be filtered.
        max_identity (float): The maximum sequence identity two sequences can share.
    Returns:
        int: The number of sequences which pass through this filtering process.
        str: The full path to the file where the filtered sequences were written.
    """
    if alignment_fn is None:
        raise ValueError('Attempting to refine alignment before an initial alignment has been generated')
    if not os.path.isdir(filter_path):
        os.makedirs(filter_path)
    identity_filtered_fn = os.path.join(filter_path, '{}.fasta'.format(protein_id))
    if os.path.isfile(identity_filtered_fn):
        filtered_alignment = SeqAlignment(file_name=identity_filtered_fn, query_id=protein_id)
        filtered_alignment.import_alignment()
    else:
        calculator = AlignmentDistanceCalculator()
        alignment = SeqAlignment(file_name=alignment_fn, query_id=protein_id)
        alignment.import_alignment()
        try:
            distance_matrix = triu(1.0 - np.array(calculator.get_distance(alignment.alignment)), k=1)
        except KeyError as e:
            print('Removing bad sequences: {}'.format(protein_id))
            alignment = alignment.remove_bad_sequences()
            distance_matrix = triu(1.0 - np.array(calculator.get_distance(alignment.alignment)), k=1)
        to_keep = set()
        to_remove = set()
        query_seq_pos = alignment.seq_order.index(protein_id)
        to_keep.add(query_seq_pos)
        for i in range(alignment.size):
            if (i in to_remove) or (i in to_keep):
                continue
            row_ids = distance_matrix[i, :]
            above_max_id = row_ids > max_identity
            positions_to_remove = set(nonzero(above_max_id)[0])
            if not positions_to_remove.isdisjoint(to_keep):
                to_remove.add(i)
            else:
                to_keep.add(i)
                to_remove.update(positions_to_remove)
        filtering_count = (len(to_keep) + len(to_remove))
        if alignment.size != filtering_count:
            raise ValueError('Identity filtering does not match alignment size {} != {} = {} + {}'.format(
                alignment.size, filtering_count, len(to_keep), len(to_remove)))
        filtered_alignment = alignment.generate_sub_alignment([alignment.seq_order[x] for x in to_keep])
        filtered_alignment.write_out_alignment(file_name=identity_filtered_fn)
    count = filtered_alignment.size
    return count, identity_filtered_fn


def init_filtering_and_alignment_pool(max_target_seqs, e_value_threshold, database, remote, min_fraction, min_identity,
                                      max_identity, msf, fasta, blast_fn, filtered_blast_path, aln_path,
                                      filtered_aln_path, final_aln_path, verbose):
    """
    Init PDB Alignment Pool

    This function is used to initialize data for a worker pool downloading PDB structures and generating alignments.

    Args:
        max_target_seqs (int): The maximum number of hits to look for in the BLAST database.
        e_value_threshold (float): The maximum e-value for a passing hit.
        database (str): The name of the database used to search for paralogs, homologs, and orthologs.
        remote (bool): Whether to perform the call to blastp remotely or not (in this case num_threads is ignored).
        min_fraction (float): The minimum fraction of the query sequence length for a passing hit.
        min_identity (float): The absolute minimum identity for a passing hit.
        max_identity (float): The absolute maximum identity for a passing hit.
        msf (bool): Whether or not to create an msf version of the MUSCLE alignment.
        fasta (bool): Whether or not to create an fasta version of the MUSCLE alignment.
        blast_fn (str): The full path to the BLAST xml containing all proteins' hits.
        filtered_blast_path (str): Directory where filtered sequences can be written in fasta format.
        aln_path (str): The directory to which the initial alignments can be written.
        filtered_aln_path (str): Path to a directory where the filtered alignment can be written.
        final_aln_path (str): The directory to which the final alignments can be written.
        verbose (bool): Whether to write out information during processing.
    """
    global max_seqs
    max_seqs = max_target_seqs
    global e_value
    e_value = e_value_threshold
    global db
    db = database
    global ncbi
    ncbi = remote
    global min_length
    min_length = min_fraction
    global min_id
    min_id = min_identity
    global max_id
    max_id = max_identity
    global make_msf
    make_msf = msf
    global make_fasta
    make_fasta = fasta
    global blast_file
    blast_file = blast_fn
    global filtered_blast_dir
    filtered_blast_dir = filtered_blast_path
    global aln_dir
    aln_dir = aln_path
    global filtered_aln_dir
    filtered_aln_dir = filtered_aln_path
    global final_aln_dir
    final_aln_dir = final_aln_path
    global verbose_out
    verbose_out = verbose


def filtering_and_alignment(in_tup):
    """
    Filtering And Alignment

    This method filters the BLAST hits found for the specified protein, aligns them, filters the alignment for identity,
    and generates a final alignment.

    Args:
        in_tup (tuple): A tuple containing the protein id and its query sequence.
            protein_id (str): The protein for which to download a PDB structure and generate an alignment.
            curr_seq (Bio.SeqRecord.SeqRecord): The query sequence for the protein id
    Returns:
        str: The protein for which data was generated.
        dict: The data and paths to the written data for the protein, the following key value pairs are stored:
            Filter_Count: (int) The number of sequences passing the filters.
            Filter_BLAST: (str) The file path to the list of sequences writen out after filtering.
            MSF_Aln: (str) The path to the initial fasta alignment produced by this method (None if fa=False).
            FA_Aln: (str) The path to the initial msf alignment produced by this method (None if msf=False).
            Final_Count: (int) The number of sequences which pass through the identity filtering process.
            Filtered_Alignment: (str) The full path to the file where the filtered alignment sequences were written.
            Final_MSF_Aln: (str) The path to the final fasta alignment produced by this method (None if fa=False).
            Final_FA_Aln: (str) The path to the final msf alignment produced by this method (None if msf=False).
        float: The time in minutes it took to generate data for this protein.
    """
    start = time()
    protein_id = in_tup[0]
    curr_seq = in_tup[1]
    curr_filter_count, curr_filter_fn = filter_blast_sequences(
        protein_id=protein_id, filter_path=filtered_blast_dir, blast_fn=blast_file, query_seq=curr_seq,
        e_value_threshold=e_value, min_fraction=min_length, min_identity=min_id, max_identity=max_id)
    msf_fn, fa_fn = align_sequences(protein_id=protein_id, alignment_path=aln_dir, pileup_fn=curr_filter_fn,
                                    msf=make_msf, fasta=make_msf, verbose=verbose_out)
    try:
        final_filter_count, final_filter_fn = identity_filter(
            protein_id=protein_id, filter_path=filtered_aln_dir, alignment_fn=fa_fn, max_identity=max_id)
    except ValueError:
        final_filter_count = 0
        final_filter_fn = None
    try:
        final_msf_fn, final_fa_fn = align_sequences(protein_id=protein_id, alignment_path=final_aln_dir,
                                                    pileup_fn=final_filter_fn, msf=make_msf, fasta=make_fasta,
                                                    verbose=verbose_out)
    except ValueError:
        final_msf_fn = None
        final_fa_fn = None
    data = {'Filter_Count': curr_filter_count, 'Filtered_BLAST': curr_filter_fn, 'MSF_Aln': msf_fn, 'FA_Aln': fa_fn,
            'Final_Count': final_filter_count, 'Filtered_Alignment': final_filter_fn, 'Final_MSF_Aln': final_msf_fn,
            'Final_FA_Aln': final_fa_fn}
    end = time()
    total = (end - start) / 60.0
    return protein_id, data, total


def determine_identity_bin(identity_count, length, interval, abs_max_identity, abs_min_identity, identity_bins):
    """
    Determine Identity Bin

    This method determines which identity bin a sequence belongs in based on the settings used for filtering
    sequences in the _restrict_sequences function.

    Args:
        identity_count (int): The number of positions which match between the query and the BLAST hit.
        length (int): The number of positions in the aligned sequence (including gaps).
        interval (int): The interval on which to define bins between min_identity and abs_min_identity in case
        sufficient sequences are not found at min_identity.
        abs_max_identity (int): The absolute maximum identity for a passing hit.
        abs_min_identity (int): The absolute minimum identity for a passing hit.
        identity_bins (set): All of the possible identity bin options.
    Returns:
        float: The identity bin in which the sequence belongs.
    """
    similarity = (identity_count / float(length)) * 100
    similarity_int = floor(similarity)
    similarity_bin = similarity_int - (similarity_int % interval)
    final_bin = None
    if abs_max_identity >= similarity_bin and similarity_bin >= abs_min_identity:
        if similarity_bin not in identity_bins and similarity_bin >= abs_min_identity:
            final_bin = abs_min_identity
        else:
            final_bin = similarity_bin
    return final_bin


def batch_iterator(iterator, batch_size):
    """
    Batch Iterator

    This can be used on any iterator, for example to batch up SeqRecord objects from Bio.SeqIO.parse(...), or to batch
    Alignment objects from Bio.AlignIO.parse(...), or simply lines from a file handle. It is a generator function, and
    it returns lists of the entries from the supplied iterator.  Each list will have
    batch_size entries, although the final list may be shorter.

    Args:
        iterator (iterable): An iterable object, in this case a parser from the Bio package is the intended input.
        batch_size (int): The maximum number of entries to return for each batch (the final batch from the iterator may
        be smaller).
    Returns:
        list: A list of length batch_size (unless it is the last batch in the iterator in which case it may be fewer) of
        entries from the provided iterator.

    Scribed from Biopython (https://biopython.org/wiki/Split_large_file)
    """
    entry = True  # Make sure we loop once
    while entry:
        batch = []
        while len(batch) < batch_size:
            try:
                entry = iterator.next()
            except StopIteration:
                entry = None
            if entry is None:
                # End of file
                break
            batch.append(entry)
        if batch:
            yield batch


def filter_uniref_fasta(in_path, out_path):
    """
    Filter Uniref Fasta

    This function can be used to filter the fasta files provided by Uniref to remove the low quality sequences and
    sequences which represent protein fragments.

    Args:
        in_path (str): The path to the fasta file to filter (should be provided by Uniref so that the expected patterns
        can be found).
        out_path (str): The path to which the filtered fasta should be written.
    """
    sequences = []
    fragment_pattern = compile(r'^.*(\(Fragment\)).*$')
    low_quality_pattern = compile(r'^.*(LOW QUALITY).*$')
    record_iter = parse(open(in_path), "fasta")
    for i, batch in enumerate(batch_iterator(record_iter, 10000)):
        print('Batch: {}'.format(i))
        for seq_record in batch:
            fragment_check = fragment_pattern.search(seq_record.description)
            low_quality_check = low_quality_pattern.search(seq_record.description)
            if fragment_check or low_quality_check:
                continue
            sequences.append(seq_record)
            if len(sequences) == 1000:
                with open(out_path, 'a') as out_handle:
                    write(sequences=sequences, handle=out_handle, format='fasta')
                sequences = []


def characterize_alignment(file_name, query_id, abs_min_fraction=0.7, abs_max_identity=98, abs_min_identity=40,
                           interval=5):
    """
    Characterize Alignment

    This function seqs to characterize a given fasta formatted alignment in the same way which the DataSetGenerator uses
    when filtering sequences returned by the BLAST search.

    Args:
        file_name (str or path): The path to the file from which the alignment can be parsed. If a relative path is used
        (i.e. the ".." prefix), python's path library will be used to attempt to define the full path.
        query_id (str): The sequence identifier of interest.
        abs_min_fraction (float): The minimum fraction of the query sequence length for a passing hit.
        abs_max_identity (int): The absolute maximum identity for a passing hit.
        abs_min_identity (int): The absolute minimum identity for a passing hit.
        interval (int): The interval on which to define bins between min_identity and abs_min_identity in case
        sufficient sequences are not found at min_identity.
    Returns:
         float: The lowest fraction sequence length (expressed as a decimal) as compared to the query sequence.
         float: The highest fraction sequence length (expressed as a decimal) as compared to the query sequence.
         set: All fraction lengths observed in the provided alignment.
         dict: A dictionary mapping the identity bins, created by the provided abs_max_identity, abs_min_identity,
         min_identity, and interval values, to lists of sequence identifiers falling in that bin.
         dict: A dictionary mapping identity values, falling outside of the computed bins, to sequence identifiers
         having those identities when compared to the query sequence.
    """
    aln = SeqAlignment(file_name=file_name, query_id=query_id)
    aln.import_alignment()
    query_pos = aln.seq_order.index(query_id)
    full_query = str(aln.alignment[query_pos].seq)
    aligned_length = len(full_query)
    ungapped_length = len(str(aln.query_sequence).replace('-', ''))
    seq_fractions = {'Low': 0, 'Passing': 0, 'High': 0}
    max_fraction = 0.0
    min_fraction = 1.0
    identity_bins = list(range(abs_min_identity, abs_max_identity, interval))
    print(identity_bins)
    sequences = {x: [] for x in identity_bins}
    out_of_range = {}
    for i in range(aln.size):
        if aln.seq_order[i] == query_id:
            continue
        full_seq = str(aln.alignment[i].seq)
        ungapped_seq = len(full_seq.replace('-', ''))
        frac1 = ungapped_seq / float(ungapped_length)
        if frac1 < min_fraction:
            min_fraction = frac1
        if frac1 > max_fraction:
            max_fraction = frac1
        subject_fraction = min(frac1, ungapped_length / float(ungapped_seq))
        if subject_fraction < abs_min_fraction:
            if ungapped_seq < ungapped_length:
                seq_fractions['Low'] += 1
            else:
                seq_fractions['High'] += 1
            continue
        else:
            seq_fractions['Passing'] += 1
        id_count = 0
        for j in range(aligned_length):
            if full_query[j] == full_seq[j]:
                id_count += 1
        similarity_bin = determine_identity_bin(identity_count=id_count, length=aligned_length, interval=interval,
                                                abs_max_identity=abs_max_identity, abs_min_identity=abs_min_identity,
                                                identity_bins=set(identity_bins))
        if similarity_bin is None:
            identity = id_count / float(aligned_length)
            if identity not in out_of_range:
                out_of_range[identity] = []
            out_of_range[identity].append(aln.seq_order[i])
        else:
            sequences[similarity_bin].append(aln.seq_order[i])
    return min_fraction, max_fraction, seq_fractions, sequences, out_of_range


def parse_arguments():
    """
    parse arguments

    This method provides a nice interface for parsing command line arguments and includes help functionality.

    Returns:
        dict. A dictionary containing the arguments parsed from the command line and their arguments.
    """
    # Create input parser
    parser = argparse.ArgumentParser(description='Process DataSetGenerator command line arguments.')
    # Arguments for creating BLAST database input from a uniref fasta
    parser.add_argument('--custom_uniref', default=False, action='store_true',
                        help='Set this argument  to create a new fasta file as input to the makeblastdb tool.')
    parser.add_argument('--original_uniref_fasta', type=str, nargs='?',
                        help='The fasta to be filtered for the makeblastdb tool')
    parser.add_argument('--filtered_uniref_fasta', type=str, nargs='?',
                        help='The path where the filtered uniref fasta file should be saved.')
    # Arguments for characterizing an existing alignment
    parser.add_argument('--characterize_alignment', default=False, action='store_true',
                        help='Set this argument  to generate statistics on an alignment.')
    parser.add_argument('--file_name', type=str, nargs='?',
                        help="The path to the file from which the alignment can be parsed. If a relative path is used "
                             "(i.e. the '..' prefix), python's path library will be used to attempt to define the full "
                             "path.")
    parser.add_argument('--query_id', type=str, nargs='?',
                        help='The sequence identifier of interest.')
    # Arguments for creating a data set using the DataSet generator
    parser.add_argument('--create_data_set', default=False, action='store_true',
                        help='Set this argument to create a data set, i.e. download and generate all input files.')
    parser.add_argument('--input_dir', type=str, nargs='?', help='The path to the directory where the data for this '
                                                                 'data set should be stored. It is expected to contain '
                                                                 'at least one directory name ProteinLists which should'
                                                                 ' contain files where each line denotes a separate PDB'
                                                                 ' id (four letter code only).')
    parser.add_argument('--protein_list_fn', type=str, nargs='?',
                        help='The name of the file where the list of PDB ids (four letter codes) of which the data set '
                             'consists can be found. Each id is expected to be on its own line.')
    parser.add_argument('--num_threads', type=int, default=1, nargs=1,
                        help='The number of threads to use when performing the BLAST search.')
    parser.add_argument('--max_target_seqs', type=int, default=20000, nargs=1,
                        help='The maximum number of hits to look for in the BLAST database.')
    parser.add_argument('--e_value_threshold', type=float, default=0.05, nargs=1,
                        help='The maximum e-value for a passing hit.')
    parser.add_argument('--min_fraction', type=float, default=0.7, nargs=1,
                        help='The minimum fraction of the query sequence length for a passing hit.')
    parser.add_argument('--min_identity', type=int, default=0.40, nargs=1,
                        help='The absolute minimum identity for a passing hit.')
    parser.add_argument('--max_identity', type=int, default=0.98, nargs=1,
                        help='The absolute maximum identity for a passing hit.')
    parser.add_argument('--msf', type=bool, default=True, nargs=1,
                        help='Whether or not to create an msf version of the MUSCLE alignment.')
    parser.add_argument('--fasta', type=bool, default=True, nargs=1,
                        help='Whether or not to create an fasta version of the MUSCLE alignment.')
    parser.add_argument('--verbose', default=False, action='store_true',
                        help='Whether or not to print out information during data set generation')
    # Clean command line input
    arguments = parser.parse_args()
    arguments = vars(arguments)
    processor_count = cpu_count()
    if arguments['num_threads'] > processor_count:
        arguments['num_threads'] = processor_count
    if arguments['custom_uniref']:
        if (not 'original_uniref_fasta' in arguments) or (not 'filtered_uniref_fasta'):
            raise ValueError('When custom_uniref is selected original_uniref_fasta and filtered_uniref_fasta must be '
                             'specfied.')
    if arguments['characterize_alignment']:
        if (not 'file_name' in arguments) or (not 'query_id' in arguments):
            raise ValueError('When characterize_alignment is selected file_name and query_id must be specified.')
    if arguments['create_data_set']:
        if (not 'input_dir' in arguments) or (not 'protein_list_fn' in arguments):
            raise ValueError('When create_data_set is selected input_dir and protein_list_fn must be specified.')
    return arguments


if __name__ == "__main__":
    args = parse_arguments()
    if args['custom_uniref']:
        filter_uniref_fasta(in_path=args['original_uniref_fasta'], out_path=args['filtered_uniref_fasta'])
    if args['characterize_alignment']:
        results = characterize_alignment(file_name=args['file_name'], query_id=args['query_id'],
                                         abs_min_fraction=args['min_fraction'],
                                         abs_max_identity=int(100 * args['max_identity']),
                                         abs_min_identity=int(100 * args['min_identity']))
        print('Sequence Fraction:\n\tMinimum:\t{}\n\tMaximum:\t{}'.format(results[0], results[1]))
        print('\t{} Sequence Fraction Low\n\t{} Sequence Fraction High\n\t{} Sequences Passed'.format(
            results[2]['Low'], results[2]['High'], results[2]['Passing']))
        print('\nSequence Identities:\n')
        for id_bin in sorted(results[3].keys()):
            print('\tBin_{}:\t{} Sequences'.format(id_bin, len(results[3][id_bin])))
        print('\nSequence Identities Outside Expected Range:\n')
        for id_bin in sorted(results[4].keys()):
            print('\tBin_{}:\t{} Sequences'.format(id_bin, len(results[4][id_bin])))
    if args['create_data_set']:
        generator = DataSetGenerator(input_path=args['input_dir'])
        generator.build_pdb_alignment_dataset(
            protein_list_fn=args['protein_list_fn'], num_threads=args['num_threads'],
            max_target_seqs=args['max_target_seqs'], e_value_threshold=args['e_value_threshold'],
            min_fraction=args['min_fraction'], min_identity=args['min_identity'],
            max_identity=args['abs_max_identity'], msf=args['msf'], fasta=args['fasta'], verbose=args['verbose'])

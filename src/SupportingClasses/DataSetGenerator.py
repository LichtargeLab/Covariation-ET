"""
Created on May 23, 2019

@author: Daniel Konecki
"""
import os
import argparse
from re import compile
import numpy as np
from numpy import floor, mean, triu, nonzero
from multiprocessing import cpu_count
from Bio.Seq import Seq
from Bio import AlignIO
from Bio.Blast import NCBIXML
from Bio.SeqIO import write, parse
from Bio.SeqRecord import SeqRecord
from Bio.PDB.PDBList import PDBList
from Bio.PDB.PDBParser import PDBParser
from Bio.Alphabet.IUPAC import ExtendedIUPACProtein
from Bio.PDB.Polypeptide import three_to_one, is_aa
from Bio.Align.Applications import MuscleCommandline, ClustalwCommandline
from Bio.Blast.Applications import NcbiblastpCommandline
from dotenv import find_dotenv, load_dotenv
from SeqAlignment import SeqAlignment
from AlignmentDistanceCalculator import AlignmentDistanceCalculator
try:
    dotenv_path = find_dotenv(raise_error_if_not_found=True)
except IOError:
    dotenv_path = find_dotenv(raise_error_if_not_found=True, usecwd=True)
load_dotenv(dotenv_path)


class DataSetGenerator(object):
    """
    This class is meant to automate data set generation and improve reproducibility of papers from our lab which rely on
    PDB structures and alignments generated by BLASTING the proteins represented in those structures.

    Attributes:
        input_path (str/path): The path to the directory where the data for this data set should be stored. It is
        expected to contain at least one directory name ProteinLists which should contain files where each line denotes
        a separate PDB id (four letter code only).
        file_name (str): The file name of the list in the ProteinLists folder (described above) which will be used to
        generate the data set for analysis. The file is expected to have a single (four letter) PDB coe on each line.
        protein_data (dict): A dictionary to hold the protein data generated over the course of data set construction.
        Initially it only contains the PDB ids parsed in from the provided file_name as keys, referencing empty
        dictionaries as values.
    """

    def __init__(self, input_path):
        """
        Init

        This function overwrites the default init function for the DataSetGenerator class.

        Args:
            input_path (path): The path to the directory where the data for this data set should be stored. It is
            expected to contain at least one directory name ProteinLists which should contain files where each line
            denotes a separate PDB id (four letter code only).
        """
        if input_path is None:
            self.input_path = os.path.join(os.environ.get('PROJECT_PATH'), 'Input')
        else:
            self.input_path = input_path
        self.protein_list_path = os.path.join(self.input_path, 'ProteinLists')
        self.pdb_path = os.path.join(self.input_path, 'PDB')
        self.protein_data = None

    def build_pdb_alignment_dataset(self, protein_list_fn, num_threads=1, max_target_seqs=20000, e_value_threshold=0.05,
                                    min_fraction=0.7, max_fraction=1.5, min_identity=75, abs_max_identity=95,
                                    abs_min_identity=30, interval=5, ignore_filter_size=False, msf=True, fasta=True):
        """
        Build Dataset

        This method builds a complete data set based on the protein id list specified in the constructor.

        Args:
            protein_list_fn (str): The name of the file where the list of PDB ids (four letter codes) of which the data set
            consists can be found. Each id is expected to be on its own line.
            num_threads (int): The number of threads to use when performing the BLAST search.
            max_target_seqs (int): The maximum number of hits to look for in the BLAST database.
            e_value_threshold (float): The maximum e-value for a passing hit.
            min_fraction (float): The minimum fraction of the query sequence length for a passing hit.
            max_fraction (float): The maximum fraction of the query sequence length for a passing hit.
            min_identity (int): The preferred minimum identity for a passing hit.
            abs_max_identity (int): The absolute maximum identity for a passing hit.
            abs_min_identity (int): The absolute minimum identity for a passing hit.
            interval (int): The interval on which to define bins between min_identity and abs_min_identity in case
            sufficient sequences are not found at min_identity.
            ignore_filter_size (bool): Whether or not to ignore the 125 sequence requirement before writing the filtered
            sequences to file.
            msf (bool): Whether or not to create an msf version of the MUSCLE alignment.
            fasta (bool): Whether or not to create an fasta version of the MUSCLE alignment.
        """
        protein_list_fn = os.path.join(self.protein_list_path, protein_list_fn)
        if not os.path.isfile(protein_list_fn):
            raise ValueError('Protein list file not cannot be found at specified location:\n{}'.format(protein_list_fn))
        self.protein_data = import_protein_list(protein_list_fn=os.path.join(self.protein_list_path, protein_list_fn))
        for seq_id in self.protein_data:
            self._download_pdb(protein_id=seq_id)
            self._parse_query_sequence(protein_id=seq_id)
            self._blast_query_sequence(protein_id=seq_id, num_threads=num_threads, max_target_seqs=max_target_seqs)
            self._restrict_sequences(protein_id=seq_id, e_value_threshold=e_value_threshold, min_fraction=min_fraction,
                                     max_fraction=max_fraction, min_identity=min_identity,
                                     abs_max_identity=abs_max_identity, abs_min_identity=abs_min_identity,
                                     interval=interval, ignore_filter_size=ignore_filter_size)
            self._align_sequences(protein_id=seq_id, msf=msf, fasta=fasta)

    # def generate_protein_data(self, protein_id):

    def _parse_query_sequence(self, protein_id):
        """
        Parse Query Sequence

        This function opens the downloaded PDB file for a given protein id (for which _download_pdb has already been
        called) and extracts the sequence of chain A for the structure. The parsed sequence is given in single letter
        amino acid codes. The sequence is saved to a file in a subdirectory of input_path with the name Sequences and a
        file name with the pattern {protein id}.fasta.

        Args:
            protein_id (str): Four letter code for a PDB id whose sequence should be parsed.
        Returns:
            str: The sequence parsed from the PDB file of the specified protein id.
            int: The length of the parsed sequence.
            str: The file path to the fasta file where the sequence has been written.
        """
        protein_fasta_path = os.path.join(self.input_path, 'Sequences')
        if not os.path.isdir(protein_fasta_path):
            os.makedirs(protein_fasta_path)
        protein_fasta_fn = os.path.join(protein_fasta_path, '{}.fasta'.format(protein_id))
        if os.path.isfile(protein_fasta_fn):
            with open(protein_fasta_fn, 'rb') as protein_fasta_handle:
                seq_iter = parse(handle=protein_fasta_handle, format='fasta')
                sequence = seq_iter.next()
                sequence.alphabet = ExtendedIUPACProtein
        else:
            parser = PDBParser(PERMISSIVE=1)  # corrective
            structure = parser.get_structure(protein_id, self.protein_data[protein_id]['PDB_Path'])
            model = structure[0]
            chain = model['A']
            sequence = []
            for residue in chain:
                if is_aa(residue.get_resname(), standard=True):
                    res_name = three_to_one(residue.get_resname())
                    sequence.append(res_name)
            sequence = SeqRecord(Seq(''.join(sequence), alphabet=ExtendedIUPACProtein), id=protein_id,
                                 description='Target Query')
            seq_records = [sequence]
            with open(protein_fasta_fn, 'wb') as protein_fasta_handle:
                write(sequences=seq_records, handle=protein_fasta_handle, format='fasta')
        self.protein_data[protein_id]['Query_Sequence'] = sequence
        self.protein_data[protein_id]['Sequence_Length'] = len(sequence)
        self.protein_data[protein_id]['Fasta_File'] = protein_fasta_fn
        return sequence, len(sequence), protein_fasta_fn

    def _blast_query_sequence(self, protein_id, num_threads=1, max_target_seqs=20000, database='nr', remote=False):
        """
        BlAST Query Sequence

        This function uses a local instance of the BLAST tool and the uniref 90 database to search for homologs and
        orthologs of the specified protein. The blast results are stored in a subdirectory of the input_path named BLAST
        with a file name following the pattern {protein id}.xml. This method assumes that _parse_query_sequence has
        already been performed for the specified protein id.

        Args:
            protein_id (str): Four letter code for the PDB id whose sequence should be searched using BLAST.
            num_threads (int): The number of threads to use when performing the BLAST search.
            max_target_seqs (int): The maximum number of hits to look for in the BLAST database.
            database (str): The name of the database used to search for paralogs, homologs, and orthologs.
            remote (bool): Whether to perform the call to blastp remotely or not (in this case num_threads is ignored).
        Returns:
            str: The path to the xml file storing the BLAST output.
        """
        blast_path = os.path.join(self.input_path, 'BLAST')
        if not os.path.isdir(blast_path):
            os.makedirs(blast_path)
        blast_fn = os.path.join(blast_path, '{}.xml'.format(protein_id))
        if not os.path.isfile(blast_fn):
            if remote:
                blastp_cline = NcbiblastpCommandline(cmd=os.path.join(os.environ.get('BLAST_PATH'), 'blastp'),
                                                     out=blast_fn, query=self.protein_data[protein_id]['Fasta_File'],
                                                     outfmt=5, remote=True, ungapped=False,
                                                     max_target_seqs=max_target_seqs,
                                                     db=os.path.join(os.environ.get('BLAST_DB_PATH'), database))
            else:
                blastp_cline = NcbiblastpCommandline(cmd=os.path.join(os.environ.get('BLAST_PATH'), 'blastp'),
                                                     out=blast_fn, query=self.protein_data[protein_id]['Fasta_File'],
                                                     outfmt=5, remote=False, ungapped=False, num_threads=num_threads,
                                                     max_target_seqs=max_target_seqs,
                                                     db=os.path.join(os.environ.get('BLAST_DB_PATH'), database))
            print(blastp_cline)
            stdout, stderr = blastp_cline()
            print(stdout)
            print(stderr)
        self.protein_data[protein_id]['BLAST_File'] = blast_fn
        return blast_fn

    def _restrict_sequences(self, protein_id, e_value_threshold=0.05, min_fraction=0.7, max_fraction=1.5,
                            min_identity=75, abs_max_identity=95, abs_min_identity=30, interval=5,
                            ignore_filter_size=False):
        """
        Restrict Sequences

        This method reads in the sequences found in a BLAST search for a given protein id. It then filters the sequences
        to make sure that there are no fragments or LOW QUALITY sequences (as defined by uniref90 which is the default
        BLAST database for this pipeline). The BLAST results are also filtered such that the e-value must be less than
        or equal to the specified cutoff (this should be done in the _blast_query_method already but it is checked here
        for completeness an in case a BLAST query is performed outside of this pipeline but used to generate a data
        set).It also filters sequences ensuring that they are all within the min_fraction and max_fraction, i.e. if you
        divide the sequence in question by the query sequence length is it within the specified range. Finally, the
        method filters based on sequence identity. It tests the identity of a sequence and if it is within the range
        covered by abs_max_identity and abs_min_identity it is placed in a bin. The first bin is in the range from
        abs_max_identity to min_identity and all other bins are intervals specified by interval between min_identity and
        abs_min_identity. These bins are combined (from highest identity, e.g. min_identity, to lowest identity, e.g.
        abs_min_identity) until at least 125 sequences have been accumulated, which are then written to file. If this
        cannot be achieved after considering all bins no restricted sequence set is written to file. If
        ignore_filter_size is set then the 125 sequence requirement is ignored.

        Args:
            protein_id (str): Four letter code for the PDB id whose BLAST search results should be filtered.
            e_value_threshold (float): The maximum e-value for a passing hit.
            min_fraction (float): The minimum fraction of the query sequence length for a passing hit.
            max_fraction (float): The maximum fraction of the query sequence length for a passing hit.
            min_identity (int): The preferred minimum identity for a passing hit.
            abs_max_identity (int): The absolute maximum identity for a passing hit.
            abs_min_identity (int): The absolute minimum identity for a passing hit.
            interval (int): The interval on which to define bins between min_identity and abs_min_identity in case
            sufficient sequences are not found at min_identity.
            ignore_filter_size (bool): Whether or not to ignore the 125 sequence requirement before writing the filtered
            sequences to file.
        Returns:
            float: The minimum identity for a passing hit used to find at least 125 passing sequences.
            int: The number of sequences passing the filters at the minimum identity (described in the first return).
            str: The file path to the list of sequences writen out after filtering, None if less than 125 sequences were
            left after filtering.

        Notes From Benu:
        Database used: A custom Uniprot 90 database using BLAST (Altschul et al. 1990).
        The sequences were aligned using MUSCLE with default parameters (Edgar 2004) and this alignment in FASTA format
        served as an input for cET-MIp
        Homologs were restricted to meet the following criteria:
            1. length of homolog within a 1.5 fractional length of query
            2. If 125+ sequences were not obtained as homologs, we relaxed the sequence identity values from 95 to 75 to
            a lower limit of >50%, >42%, and >30% (e.g. Sung 2016 paper). We also limited the
            3. Lastly, we restricted the hits to those with a maximum e-val of 0.05 and removed putative and incomplete
            sequences. Fractional length to define short sequences was kept at 0.7-0.8 wrt query length
        """
        pileup_path = os.path.join(self.input_path, 'Pileups')
        if not os.path.isdir(pileup_path):
            os.makedirs(pileup_path)
        pileup_fn = os.path.join(pileup_path, '{}.fasta'.format(protein_id))
        identity_bins = list(range(abs_min_identity, min_identity, interval))
        sequences = {x: [] for x in identity_bins}
        if min_identity not in sequences:
            identity_bins.append(min_identity)
            sequences[min_identity] = []
        fragment_pattern = compile(r'^.*(\(Fragment\)).*$')
        low_quality_pattern = compile(r'^.*(LOW QUALITY).*$')
        if os.path.isfile(pileup_fn):
            self.protein_data[protein_id]['Pileup_File'] = pileup_fn
            hsp_data_pattern = compile(r'^.*\sHSP_identity=(\d+)\sHSP_alignment_length=(\d+).*$')
            with open(pileup_fn, 'rb') as pileup_handle:
                fasta_iter = parse(handle=pileup_handle, format='fasta')
                for seq_record in fasta_iter:
                    if seq_record.description.endswith('Target Query'):  # Skip the target sequence in the pileup.
                        continue
                    hsp_data_match = hsp_data_pattern.match(seq_record.description)
                    seq_id = int(hsp_data_match.group(1))
                    seq_len = int(hsp_data_match.group(2))
                    similarity_bin = determine_identity_bin(
                        identity_count=seq_id, length=seq_len, interval=interval, abs_max_identity=abs_max_identity,
                        abs_min_identity=abs_min_identity, min_identity=min_identity, identity_bins=set(identity_bins))
                    if similarity_bin:
                        seq_record.alphabet = ExtendedIUPACProtein
                        sequences[similarity_bin].append(seq_record)
        else:
            with open(self.protein_data[protein_id]['BLAST_File'], 'rb') as blast_handle:
                blast_record = NCBIXML.read(blast_handle)
                for alignment in blast_record.alignments:
                    fragment_check = fragment_pattern.search(alignment.hit_def)
                    low_quality_check = low_quality_pattern.search(alignment.hit_def)
                    if fragment_check or low_quality_check:
                        continue
                    aln_seq_record = None
                    aln_similarity_bin = None
                    for hsp in alignment.hsps:
                        if hsp.expect <= e_value_threshold:  # Should already be controlled by BLAST e-value
                            subject_length = len(hsp.sbjct)
                            subject_fraction = subject_length / float(self.protein_data[protein_id]['Sequence_Length'])
                            if (min_fraction <= subject_fraction) and (subject_fraction <= max_fraction):
                                similarity_bin = determine_identity_bin(
                                    identity_count=hsp.identities, length=hsp.align_length, interval=interval,
                                    abs_max_identity=abs_max_identity, abs_min_identity=abs_min_identity,
                                    min_identity=min_identity, identity_bins=set(identity_bins))
                                if similarity_bin:
                                    # This check has been added for alignments which have multiple HSPs so that
                                    # duplicate sequences are not saved.
                                    if (aln_similarity_bin is None) or (similarity_bin > aln_similarity_bin):
                                        new_description = '{} HSP_identity={} HSP_alignment_length={}'.format(
                                            alignment.hit_def, hsp.identities, hsp.align_length)
                                        aln_seq_record = SeqRecord(Seq(hsp.sbjct, alphabet=ExtendedIUPACProtein),
                                                                       id=alignment.hit_id, name=alignment.title,
                                                                       description=new_description)
                                        aln_similarity_bin = similarity_bin
                    if aln_similarity_bin:
                        sequences[aln_similarity_bin].append(aln_seq_record)
        final_sequences = []
        # Build up the list of sequences from each identity bin (track which one yielded the 125th sequence)
        for i in range(len(identity_bins) - 1, -1, -1):
            final_sequences += sequences[identity_bins[i]]
            if len(final_sequences) >= 125:
                final_identity_bin = identity_bins[i]
        count = len(final_sequences)
        if not os.path.isfile(pileup_fn):
            if ignore_filter_size or len(final_sequences) >= 125:
                # Add query sequence so that this file can be fed directly to the alignment method.
                final_sequences = [self.protein_data[protein_id]['Query_Sequence']] + final_sequences
                with open(pileup_fn, 'wb') as pileup_handle:
                    write(sequences=final_sequences, handle=pileup_handle, format='fasta')
            else:
                pileup_fn = None
                print('No pileup for pdb: {}, sufficient sequences could not be found'.format(protein_id))
        self.protein_data[protein_id]['Pileup_File'] = pileup_fn
        return final_identity_bin, count, pileup_fn

    def _align_sequences(self, protein_id, msf=True, fasta=True, source='Pileup_File'):
        """
        Align Sequences

        This method uses CLUSTALW to align the query sequence and all of the hits from BLAST which passed the filtering
        process by default the alignment is performed once, to produce a fasta alignment file, and then converted to
        produce the msf alignment file, however either of these options can be turned off.

        Args:
            protein_id (str): Four letter code for the PDB id for which an alignment should be performed.
            msf (bool): Whether or not to create an msf version of the MUSCLE alignment.
            fasta (bool): Whether or not to create an fasta version of the MUSCLE alignment.
        Returns:
            str: The path to the msf alignment produced by this method (None if msf=False).
            str: The path to the fasta alignment produced by this method (None if fa=False).
        """
        clustalw_path = os.environ.get('CLUSTALW_PATH')
        if self.protein_data[protein_id][source] is None:
            raise ValueError('Attempting to perform alignment before pileup file was generated.')
        alignment_path = os.path.join(self.input_path, 'Alignments')
        if not os.path.isdir(alignment_path):
            os.makedirs(alignment_path)
        fa_fn = None
        if fasta:
            fa_fn = os.path.join(alignment_path, '{}.fasta'.format(protein_id))
            if not os.path.isfile(fa_fn):
                fa_cline = ClustalwCommandline(clustalw_path, infile=self.protein_data[protein_id][source],
                                               align=True, quicktree=True, outfile=fa_fn, output='FASTA')
                print(fa_cline)
                stdout, stderr = fa_cline()
                print(stdout)
                print(stderr)
        msf_fn = None
        if msf:
            msf_fn = os.path.join(alignment_path, '{}.msf'.format(protein_id))
            if not os.path.isfile(msf_fn):
                if fasta:
                    msf_cline = ClustalwCommandline(clustalw_path, infile=fa_fn, convert=True, outfile=msf_fn,
                                                    output='GCG')
                else:
                    msf_cline = ClustalwCommandline(clustalw_path, infile=self.protein_data[protein_id][source],
                                               align=True, quicktree=True, outfile=msf_fn, output='GCG')
                print(msf_cline)
                stdout, stderr = msf_cline()
                print(stdout)
                print(stderr)
        self.protein_data[protein_id]['MSF_File'] = msf_fn
        self.protein_data[protein_id]['FA_File'] = fa_fn
        return msf_fn, fa_fn

    def _identity_filter(self, protein_id, max_identity=0.98):
        if self.protein_data[protein_id]['FA_File'] is None:
            raise ValueError('Attempting to refine alignment before an initial alignment has been generated')
        identity_filtered_path = os.path.join(self.input_path, 'Identity_Filtered')
        if not os.path.isdir(identity_filtered_path):
            os.makedirs(identity_filtered_path)
        calculator = AlignmentDistanceCalculator()
        alignment = SeqAlignment(file_name=self.protein_data[protein_id]['FA_File'], query_id=protein_id)
        alignment.import_alignment()
        distance_matrix = triu(np.array(calculator.get_distance(alignment.alignment)), k=1)
        to_keep = set()
        to_remove = set()
        query_seq_pos = alignment.seq_order.index(protein_id)
        to_keep.add(query_seq_pos)
        for i in range(alignment.size):
            if (i in to_remove) or (i in to_keep):
                continue
            row_ids = distance_matrix[i, :]
            above_max_id = row_ids > max_identity
            positions_to_remove = set(nonzero(above_max_id)[0])
            if not positions_to_remove.isdisjoint(to_keep):
                to_remove.add(i)
            else:
                to_keep.add(i)
                to_remove.update(positions_to_remove)
        filtering_count = (len(to_keep) + len(to_remove))
        if alignment.size != filtering_count:
            raise ValueError('Identity filtering does not match alignment size {} != {} = {} + {}'.format(
                alignment.size, filtering_count, len(to_keep), len(to_remove)))
        filtered_alignment = alignment.generate_sub_alignment([alignment.seq_order[x] for x in to_keep])
        identity_filtered_fn = os.path.join(identity_filtered_path, '{}.fasta'.format(protein_id))
        filtered_alignment.write_out_alignment(file_name=identity_filtered_fn)
        self.protein_data[protein_id]['Identity_Filtered_FA'] = identity_filtered_fn
        return identity_filtered_fn


def import_protein_list(protein_list_fn):
    """
    Import protein list

    This function opens the list file found at protein_list_fn and parses in the PDB ids and the chain of interest
    specified there.

    Args:
        protein_list_fn (str): The name of the file where the list of PDB ids and their chain of interest (five letter
        codes) of which the data set consists can be found. Each id is expected to be on its own line.
    Returns:
        dict: A dictionary where each key is a single PDB id parsed from the specified list file and each value is
        a dictionary with the first key and value being the specified chain of interest, and will be filled as the
        data set is constructed.
    """
    protein_list_fn = os.path.join(protein_list_fn)
    protein_list = {}
    pdb_id_pattern = compile(r'^([0-9][a-z0-9]{3})([A-Z])$')
    with open(protein_list_fn, mode='rb') as protein_list_handle:
        for line in protein_list_handle:
            pdb_id_match = pdb_id_pattern.match(line.strip())
            protein_list[pdb_id_match.group(1)] = {'Chain': pdb_id_match.group(2)}
    return protein_list


def download_pdb(pdb_path, protein_id):
    """
    Download PDB

    This function downloads the PDB structure file for the given PDB id provided. The file is stored in a file within a
    sub directory of the provided pdb_path named <pdb_path>/{middle two characters of the PDB id}/pdb{pdb id}.ent

    Args:
        pdb_path (str): The location at which PDB structures should be saved.
        protein_id (str): Four letter code for a PDB id to be downloaded.
    Returns:
        str: The path to the PDB file downloaded.
    """
    if not os.path.isdir(pdb_path):
        os.makedirs(pdb_path)
    pdb_list = PDBList(server='ftp://ftp.wwpdb.org', pdb=pdb_path)
    pdb_file = pdb_list.retrieve_pdb_file(pdb_code=protein_id, file_format='pdb')
    return pdb_file


def determine_identity_bin(identity_count, length, interval, abs_max_identity, abs_min_identity, min_identity,
                           identity_bins):
    """
    Determine Identity Bin

    This method determines which identity bin a sequence belongs in based on the settings used for filtering
    sequences in the _restrict_sequences function.

    Args:
        identity_count (int): The number of positions which match between the query and the BLAST hit.
        length (int): The number of positions in the aligned sequence (including gaps).
        interval (int): The interval on which to define bins between min_identity and abs_min_identity in case
        sufficient sequences are not found at min_identity.
        abs_max_identity (int): The absolute maximum identity for a passing hit.
        abs_min_identity (int): The absolute minimum identity for a passing hit.
        min_identity (int): The preferred minimum identity for a passing hit.
        identity_bins (set): All of the possible identity bin options.
    Returns:
        float: The identity bin in which the sequence belongs.
    """
    similarity = (identity_count / float(length)) * 100
    similarity_int = floor(similarity)
    similarity_bin = similarity_int - (similarity_int % interval)
    final_bin = None
    if abs_max_identity >= similarity_bin and similarity_bin >= abs_min_identity:
        if similarity_bin >= min_identity:
            final_bin = min_identity
        elif similarity_bin not in identity_bins and similarity_bin >= abs_min_identity:
            final_bin = abs_min_identity
        else:
            final_bin = similarity_bin
    return final_bin


def batch_iterator(iterator, batch_size):
    """
    Batch Iterator

    This can be used on any iterator, for example to batch up SeqRecord objects from Bio.SeqIO.parse(...), or to batch
    Alignment objects from Bio.AlignIO.parse(...), or simply lines from a file handle. It is a generator function, and
    it returns lists of the entries from the supplied iterator.  Each list will have
    batch_size entries, although the final list may be shorter.

    Args:
        iterator (iterable): An iterable object, in this case a parser from the Bio package is the intended input.
        batch_size (int): The maximum number of entries to return for each batch (the final batch from the iterator may
        be smaller).
    Returns:
        list: A list of length batch_size (unless it is the last batch in the iterator in which case it may be fewer) of
        entries from the provided iterator.

    Scribed from Biopython (https://biopython.org/wiki/Split_large_file)
    """
    entry = True  # Make sure we loop once
    while entry:
        batch = []
        while len(batch) < batch_size:
            try:
                entry = iterator.next()
            except StopIteration:
                entry = None
            if entry is None:
                # End of file
                break
            batch.append(entry)
        if batch:
            yield batch


def filter_uniref_fasta(in_path, out_path):
    """
    Filter Uniref Fasta

    This function can be used to filter the fasta files provided by Uniref to remove the low quality sequences and
    sequences which represent protein fragments.

    Args:
        in_path (str): The path to the fasta file to filter (should be provided by Uniref so that the expected patterns
        can be found).
        out_path (str): The path to which the filtered fasta should be written.
    """
    sequences = []
    fragment_pattern = compile(r'^.*(\(Fragment\)).*$')
    low_quality_pattern = compile(r'^.*(LOW QUALITY).*$')
    record_iter = parse(open(in_path), "fasta")
    for i, batch in enumerate(batch_iterator(record_iter, 10000)):
        print('Batch: {}'.format(i))
        for seq_record in batch:
            fragment_check = fragment_pattern.search(seq_record.description)
            low_quality_check = low_quality_pattern.search(seq_record.description)
            if fragment_check or low_quality_check:
                continue
            sequences.append(seq_record)
            if len(sequences) == 1000:
                with open(out_path, 'ab') as out_handle:
                    write(sequences=sequences, handle=out_handle, format='fasta')
                sequences = []


def characterize_alignment(file_name, query_id, abs_max_identity=95, abs_min_identity=30, min_identity=75, interval=5):
    """
    Characterize Alignment

    This function seqs to characterize a given fasta formatted alignment in the same way which the DataSetGenerator uses
    when filtering sequences returned by the BLAST search.

    Args:
        file_name (str or path): The path to the file from which the alignment can be parsed. If a relative path is
        used (i.e. the ".." prefix), python's path library will be used to attempt to define the full path.
        query_id (str): The sequence identifier of interest.
        abs_max_identity (int): The absolute maximum identity for a passing hit.
        abs_min_identity (int): The absolute minimum identity for a passing hit.
        min_identity (int): The preferred minimum identity for a passing hit.
        interval (int): The interval on which to define bins between min_identity and abs_min_identity in case
        sufficient sequences are not found at min_identity.
    Returns:
         float: The lowest fraction sequence length (expressed as a decimal) as compared to the query sequence.
         float: The highest fraction sequence length (expressed as a decimal) as compared to the query sequence.
         set: All fraction lengths observed in the provided alignment.
         dict: A dictionary mapping the identity bins, created by the provided abs_max_identity, abs_min_identity,
         min_identity, and interval values, to lists of sequence identifiers falling in that bin.
         dict: A dictionary mapping identity values, falling outside of the computed bins, to sequence identifiers
         having those identities when compared to the query sequence.
    """
    aln = SeqAlignment(file_name=file_name, query_id=query_id)
    aln.import_alignment()
    query_pos = aln.seq_order.index(query_id)
    full_query = str(aln.alignment[query_pos].seq)
    aligned_length = len(full_query)
    max_fraction = 0.0
    min_fraction = 1.0
    seq_fractions = []
    identity_bins = list(range(abs_min_identity, min_identity, interval))
    sequences = {x: [] for x in identity_bins}
    out_of_range = {}
    if min_identity not in sequences:
        identity_bins.append(min_identity)
        sequences[min_identity] = []
    for i in range(aln.size):
        if aln.seq_order[i] == query_id:
            continue
        full_seq = str(aln.alignment[i].seq)
        ungapped_seq = len(full_seq.replace('-', ''))
        subject_fraction = ungapped_seq / float(len(str(aln.query_sequence).replace('-', '')))
        if subject_fraction > max_fraction:
            max_fraction = subject_fraction
        if subject_fraction < min_fraction:
            min_fraction = subject_fraction
        seq_fractions.append(subject_fraction)
        id_count = 0
        for j in range(aligned_length):
            if full_query[j] == full_seq[j]:
                id_count += 1
        similarity_bin = determine_identity_bin(identity_count=id_count, length=aligned_length, interval=interval,
                                                abs_max_identity=abs_max_identity, abs_min_identity=abs_min_identity,
                                                min_identity=min_identity, identity_bins=set(identity_bins))
        if similarity_bin is None:
            identity = id_count / float(aligned_length)
            if identity not in out_of_range:
                out_of_range[identity] = []
            out_of_range[identity].append(aln.seq_order[i])
        else:
            sequences[similarity_bin].append(aln.seq_order[i])
    return min_fraction, max_fraction, seq_fractions, sequences, out_of_range


def parse_arguments():
    """
    parse arguments

    This method provides a nice interface for parsing command line arguments and includes help functionality.

    Returns:
        dict. A dictionary containing the arguments parsed from the command line and their arguments.
    """
    # Create input parser
    parser = argparse.ArgumentParser(description='Process DataSetGenerator command line arguments.')
    # Arguments for creating BLAST database input from a uniref fasta
    parser.add_argument('--custom_uniref', default=False, action='store_true',
                        help='Set this argument  to create a new fasta file as input to the makeblastdb tool.')
    parser.add_argument('--original_uniref_fasta', type=str, nargs='?',
                        help='The fasta to be filtered for the makeblastdb tool')
    parser.add_argument('--filtered_uniref_fasta', type=str, nargs='?',
                        help='The path where the filtered uniref fasta file should be saved.')
    # Arguments for characterizing an existing alignment
    parser.add_argument('--characterize_alignment', default=False, action='store_true',
                        help='Set this argument  to generate statistics on an alignment.')
    parser.add_argument('--file_name', type=str, nargs='?',
                        help="The path to the file from which the alignment can be parsed. If a relative path is used "
                             "(i.e. the '..' prefix), python's path library will be used to attempt to define the full "
                             "path.")
    parser.add_argument('--query_id', type=str, nargs='?',
                        help='The sequence identifier of interest.')
    # Arguments for creating a data set using the DataSet generator
    parser.add_argument('--create_data_set', default=False, action='store_true',
                        help='Set this argument to create a data set, i.e. download and generate all input files.')
    parser.add_argument('--input_dir', type=str, nargs='?', help='The path to the directory where the data for this '
                                                                 'data set should be stored. It is expected to contain '
                                                                 'at least one directory name ProteinLists which should'
                                                                 ' contain files where each line denotes a separate PDB'
                                                                 ' id (four letter code only).')
    parser.add_argument('--protein_list_fn', type=str, nargs='?',
                        help='The name of the file where the list of PDB ids (four letter codes) of which the data set '
                             'consists can be found. Each id is expected to be on its own line.')
    parser.add_argument('--num_threads', type=int, default=1, nargs=1,
                        help='The number of threads to use when performing the BLAST search.')
    parser.add_argument('--max_target_seqs', type=int, default=20000, nargs=1,
                        help='The maximum number of hits to look for in the BLAST database.')
    parser.add_argument('--e_value_threshold', type=float, default=0.05, nargs=1,
                        help='The maximum e-value for a passing hit.')
    parser.add_argument('--min_fraction', type=float, default=0.7, nargs=1,
                        help='The minimum fraction of the query sequence length for a passing hit.')
    parser.add_argument('--max_fraction', type=float, default=1.5, nargs=1,
                        help='The maximum fraction of the query sequence length for a passing hit.')
    parser.add_argument('--min_identity', type=int, default=75, nargs=1,
                        help='The preferred minimum identity for a passing hit.')
    parser.add_argument('--abs_max_identity', type=int, default=95, nargs=1,
                        help='The absolute maximum identity for a passing hit.')
    parser.add_argument('--abs_min_identity', type=int, default=30, nargs=1,
                        help='The absolute minimum identity for a passing hit.')
    parser.add_argument('--interval', type=int, default=5, nargs=1,
                        help='The interval on which to define bins between min_identity and abs_min_identity in case '
                             'sufficient sequences are not found at min_identity.')
    parser.add_argument('--ignore_filter_size', type=bool, default=False, nargs=1,
                        help='Whether or not to ignore the 125 sequence requirement before writing the filtered '
                             'sequences to file.')
    parser.add_argument('--msf', type=bool, default=True, nargs=1,
                        help='Whether or not to create an msf version of the MUSCLE alignment.')
    parser.add_argument('--fasta', type=bool, default=True, nargs=1,
                        help='Whether or not to create an fasta version of the MUSCLE alignment.')
    # Clean command line input
    arguments = parser.parse_args()
    arguments = vars(arguments)
    processor_count = cpu_count()
    if arguments['num_threads'] > processor_count:
        arguments['num_threads'] = processor_count
    if arguments['custom_uniref']:
        if (not 'original_uniref_fasta' in arguments) or (not 'filtered_uniref_fasta'):
            raise ValueError('When custom_uniref is selected original_uniref_fasta and filtered_uniref_fasta must be specfied.')
    if arguments['characterize_alignment']:
        if (not 'file_name' in arguments) or (not 'query_id' in arguments):
            raise ValueError('When characterize_alignment is selected file_name and query_id must be specified.')
    if arguments['create_data_set']:
        if (not 'input_dir' in arguments) or (not 'protein_list_fn' in arguments):
            raise ValueError('When create_data_set is selected input_dir and protein_list_fn must be specified.')
    return arguments


if __name__ == "__main__":
    args = parse_arguments()
    if args['custom_uniref']:
        filter_uniref_fasta(in_path=args['original_uniref_fasta'], out_path=args['filtered_uniref_fasta'])
    if args['characterize_alignment']:
        res = characterize_alignment(file_name=args['file_name'], query_id=args['query_id'],
                                     abs_max_identity=args['abs_max_identity'],
                                     abs_min_identity=args['abs_min_identity'], min_identity=args['min_identity'],
                                     interval=args['interval'])
        print('Sequence Fraction:\n\tMinimum:\t{}\n\tMaximum:\t{}\n\tAverage:\t{}'.format(res[0], res[1], mean(res[2])))
        print('\nSequence Identities:\n')
        for id_bin in sorted(res[3].keys()):
            print('\tBin_{}:\t{} Sequences'.format(id_bin, len(res[3][id_bin])))
        print('\nSequence Identities Outside Expected Range:\n')
        for id_bin in sorted(res[4].keys()):
            print('\tBin_{}:\t{} Sequences'.format(id_bin, len(res[4][id_bin])))
    if args['create_data_set']:
        generator = DataSetGenerator(protein_list=args['protein_list_fn'], input_path=args['input_dir'])
        generator.build_dataset(num_threads=args['num_threads'], max_target_seqs=args['max_target_seqs'],
                                e_value_threshold=args['e_value_threshold'], min_fraction=args['min_fraction'],
                                max_fraction=args['max_fraction'], min_identity=args['min_identity'],
                                abs_max_identity=args['abs_max_identity'], abs_min_identity=args['abs_min_identity'],
                                interval=args['interval'], ignore_filter_size=args['ignore_filter_size'],
                                msf=args['msf'], fasta=args['fasta'])